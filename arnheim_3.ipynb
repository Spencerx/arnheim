{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spHtkST4105t"
      },
      "source": [
        "Copyright 2021 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfSc66QYgnfR"
      },
      "source": [
        "# Arnheim 3 - Collage\n",
        "\n",
        "**Piotr Mirowski, Dylan Banarse, Mateusz Malinowski, Yotam Doron, Oriol Vinyals, Simon Osindero, Chrisantha Fernando**\n",
        "\n",
        "DeepMind, 2021\n",
        "\n",
        "![picture](https://github.com/deepmind/arnheim/raw/main/images/arnheim3_examples.png)\n",
        "Clockwise from top left: \"Sri Lankan objects\" (200 transparent patches); \"Waves\" (70 masked transparnecy patches with background); \"Fruit bowl\" (100 opacity patches); \"Fruit bowl\" (100 transparent patches); \"Face\" (7 opacity patches); \"Swans\" (100 masked transparent patches); \"Chicken\" (70 masked transparent patches); \"Dancer\" (40 transparent patches). See description in the [videos](https://www.youtube.com/watch?v=HKDQsrO5xF4&list=PLKhLdFXp1JN5SEV56w9OWWsT5pAz9z7G_) for settings.\n",
        "\n",
        "**STEPS:**\n",
        "\n",
        "Quickstart:\n",
        "1. Click \"Connect\" in the top right corner\n",
        "1. Runtime -> Run all\n",
        "\n",
        "Play around:\n",
        "* Parameters section to configure collage creation settings\n",
        "* Image patches section to upload your own patches\n",
        "* Make Drawing section to set the text prompt\n",
        "\n",
        "**An Exploration of Architectures and Losses for Painting and Drawing**\n",
        "\n",
        "Arnheim 3 is an algorithm which generates collages by training by gradient descent a network which applies affine transformations, i.e translation, scaling, rotation, and shear to a set of image patches, this set being subject to evolution in the outer loop. \n",
        "\n",
        "The signal for how good an image is comes from CLIP, a text-image dual encoder. This work simplifies and extends Arnheim 2 which also used CLIP but generated SVG strokes using a more complex hierarchical stroke grammar. \n",
        "\n",
        "Here you can experiment with a variety of rendering methods for combining patches in a learnable way.\n",
        "\n",
        "**New Features**\n",
        "1. Compositional Images\n",
        "\n",
        "  Uses 3x3 prompts covering over-lapping regions of the image to specify different content across the whole image. The main prompt guides the direction of overall image.\n",
        "\n",
        "1. Coloured Background\n",
        "\n",
        "  User-selectable background colour or use of uploaded images.\n",
        "\n",
        "1. Interactive Patch Placement\n",
        "\n",
        "  Stop the \"Create collage! (Loop)\" cell at any time and run the \"Tinker with patches\" cell below it to adjust individual patches with sliders. Then re-run the \"Create collage! (Loop)\" cell to continue generation.\n",
        "\n",
        "**Tips**\n",
        "\n",
        "**opacity rendering** uses alpha and depth to render semi-opaque overlapping patches which allow gradients to be used during learning. The translucency is reduced over the course of learning to end with opaque patches. When using a small number of patches evolution can perform better than learning alone. For example, with only 7 patches a population of 10 with the Evolutionary Strategies method applied at every step can yield good results. The settings to get the face image above were:\n",
        "\n",
        "* Prompt “Face”\n",
        "* 7 patches\n",
        "* opacity\n",
        "* 400 steps\n",
        "* ES evolution every step\n",
        "* POP_SIZE=10\n",
        "* ROT_POS_MUTATION=0.05\n",
        "* SCALE_MUTATION = 0.02\n",
        "* PATCH_MUTATION = 0.2\n",
        "\n",
        "**Transparency rendering** works well as gradients are more effective. Note that colours are additive so setting INITIAL_MIN_RGB=0.1 and INITIAL_MAX_RGB=0.5 helps reduce bleaching. Something to try is:\n",
        "\n",
        "* 80 Patches\n",
        "* Transparency\n",
        "* INITIAL_MIN_RGB=0.1; INITIAL_MAX_RGB=0.5\n",
        "* 15000 steps\n",
        "* Microbial GA every 100 steps\n",
        "* POP_SIZE = 2\n",
        "* LEARNING_RATE = 0.07\n",
        "* PATCH_MUTATION_PROBABILITY = 1\n",
        "* Prompt \"Swans on a pond\"\n",
        "\n",
        "**Masked Transparency rendering** also works well as gradients are more effective.  Something to try is:\n",
        "\n",
        "* 200 Patches\n",
        "* MASKED Transparency\n",
        "* INITIAL_MIN_RGB=0.7; INITIAL_MAX_RGB=1.0\n",
        "* 15000 steps\n",
        "* Microbial GA every 100 steps\n",
        "* POP_SIZE = 2\n",
        "* LEARNING_RATE = 0.07\n",
        "* PATCH_MUTATION_PROBABILITY = 1\n",
        "* Prompt \"Swans on a pond\"\n",
        "\n",
        "**Note that the Colab can easily run out of memory with large populations, many patches and large patch sizes! If you start to encounter CUDA memory issues try lowering the number of patches and restarting the Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEKdIbK4VZa"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5dyyH781qzIC"
      },
      "outputs": [],
      "source": [
        "#@title Installation of libraries {vertical-output: true}\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "  torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "  torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "  torch_version_suffix = \"\"\n",
        "else:\n",
        "  torch_version_suffix = \"+cu110\"\n",
        "\n",
        "%cd /content/\n",
        "!pip install cssutils\n",
        "!pip install torch-tools\n",
        "!pip install   visdom\n",
        "!pip install kornia\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git --no-deps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bdUyJs4hq3"
      },
      "source": [
        "# Imports and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MGB-vb1MIXDd"
      },
      "outputs": [],
      "source": [
        "#@title Imports {vertical-output: true}\n",
        "import clip\n",
        "import copy\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "import io\n",
        "from kornia.color import hsv\n",
        "from matplotlib import pyplot as plt\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import requests\n",
        "from skimage.transform import resize\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "os.environ[\"FFMPEG_BINARY\"] = \"ffmpeg\"\n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yIVTTQO-lLCx"
      },
      "outputs": [],
      "source": [
        "#@title Initialise and load CLIP model {vertical-output: true}\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "CLIP_MODEL = \"ViT-B/32\"\n",
        "print(f\"Downloading CLIP model {CLIP_MODEL}...\")\n",
        "clip_model, _ = clip.load(CLIP_MODEL, device, jit=False)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlVNTb_1NelG"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j70Ff0qRNBjh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Collage configuration\n",
        "#@markdown Render methods\n",
        "\n",
        "#@markdown \n",
        "#@markdown **opacity** patches overlay each other using a combination of alpha and depth,\n",
        "#@markdown **transparency** _adds_ patch colours (black therefore appearing transparent),\n",
        "#@markdown and **masked transparency** blends patches using the alpha channel.\n",
        "RENDER_METHOD = \"masked_transparency\"  #@param [\"opacity\", \"transparency\", \"masked_transparency\"]\n",
        "NUM_PATCHES =      100#@param {type:\"integer\"}\n",
        "COLOUR_TRANSFORMATIONS = \"RGB space\"  #@param [\"none\", \"RGB space\", \"HSV space\"]\n",
        "#@markdown Invert image colours to have a white background?\n",
        "INVERT_COLOURS = False #@param {type:\"boolean\"}\n",
        "\n",
        "CANVAS_WIDTH = 224\n",
        "CANVAS_HEIGHT = 224\n",
        "MULTIPLIER_BIG_IMAGE = 4\n",
        "\n",
        "#@markdown Use additional prompts for different regions\n",
        "COMPOSITIONAL_IMAGE = True #@param {type:\"boolean\"}\n",
        "if COMPOSITIONAL_IMAGE:\n",
        "  CANVAS_WIDTH *= 2\n",
        "  CANVAS_HEIGHT *= 2\n",
        "  MULTIPLIER_BIG_IMAGE = int(MULTIPLIER_BIG_IMAGE / 2)\n",
        "\n",
        "PATCH_WIDTH_MIN = 16  \n",
        "PATCH_HEIGHT_MIN = 16 \n",
        "PATCH_MAX_PROPORTION =  3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5vRneu99YwV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Affine transform settings\n",
        "\n",
        "#@markdown Translation bounds for X and Y.\n",
        "MIN_TRANS = -1  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "MAX_TRANS = 1  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "#@markdown Scale bounds (> 1 means zoom out and < 1 means zoom in).\n",
        "MIN_SCALE =   0.5#@param {type:\"number\"}\n",
        "MAX_SCALE =   2.0#@param {type:\"number\"\n",
        "#@markdown Bounds on ratio between X and Y scale (default 1).\n",
        "MIN_SQUEEZE =   0.5#@param {type:\"number\"}\n",
        "MAX_SQUEEZE =   2.0#@param {type:\"number\"}\n",
        "#@markdown Shear deformation bounds (default 0)\n",
        "MIN_SHEAR = -0.2  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "MAX_SHEAR = 0.2  #@param{type:\"slider\", min:-1.0, max:1.0, step:0.01}\n",
        "#@markdown Rotation bounds.\n",
        "MIN_ROT_DEG = -180 #@param{type:\"slider\", min:-180, max:180, step:1}\n",
        "MAX_ROT_DEG = 180 #@param{type:\"slider\", min:-180, max:180, step:1}\n",
        "MIN_ROT = MIN_ROT_DEG * np.pi / 180.0\n",
        "MAX_ROT = MAX_ROT_DEG * np.pi / 180.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMn71brOJb1Z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Colour transform settings\n",
        "\n",
        "#@markdown RGB\n",
        "MIN_RGB = -0.45  #@param {type:\"slider\", min: -1, max: 1, step: 0.01}\n",
        "MAX_RGB = 1.0  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "INITIAL_MIN_RGB = 0.05  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "INITIAL_MAX_RGB = 0.4  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "#@markdown HSV\n",
        "MIN_HUE = 0.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "MAX_HUE_DEG = 360 #@param {type:\"slider\", min: 0, max: 360, step: 1}\n",
        "MAX_HUE = MAX_HUE_DEG * np.pi / 180.0\n",
        "MIN_SAT = 0.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "MAX_SAT = 1.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "MIN_VAL = 0.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}\n",
        "MAX_VAL = 1.  #@param {type:\"slider\", min: 0, max: 1, step: 0.01}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFZ31zE0AAKJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training settings\n",
        "\n",
        "# Reasonable defaults:\n",
        "# OPTIM_STEP = 10000 to 20000\n",
        "# LEARNING_RATE = 0.1\n",
        "# NUM_AUGS = 2 to 4\n",
        "# GRADIENT_CLIPPING = 10.0\n",
        "# USE_NORMALIZED_CLIP = True\n",
        "\n",
        "#@markdown Number of training steps\n",
        "OPTIM_STEPS = 10000    #@param{type:\"slider\", min:200, max:20000, step:100}\n",
        "\n",
        "LEARNING_RATE = 0.1    #@param{type:\"slider\", min:0.0, max:0.3, step:0.001}\n",
        "#@markdown Number of augmentations to use in evaluation\n",
        "USE_IMAGE_AUGMENTATIONS = True #@param{type:\"boolean\"}\n",
        "NUM_AUGS = 4  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Normalize colours for CLIP, generally leave this as True\n",
        "USE_NORMALIZED_CLIP = False  #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Gradient clipping during optimisation\n",
        "GRADIENT_CLIPPING = 10.0  #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Initial random search size (1 means no search)\n",
        "INITIAL_SEARCH_SIZE = 1 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "if COMPOSITIONAL_IMAGE:\n",
        "  print(\"Using ONE image augmentations for compositional image creation.\")\n",
        "  USE_IMAGE_AUGMENTATIONS = True\n",
        "  NUM_AUGS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-yFYf0vAS42",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Evolution settings\n",
        "\n",
        "# Reasonable defaults:\n",
        "# POP_SIZE = 2\n",
        "# EVOLUTION_FREQUENCY = 100\n",
        "# MUTION SCALES = ~0.1\n",
        "# MAX_MULTIPLE_VISUALISATIONS = 7\n",
        "\n",
        "#@markdown For evolution set POP_SIZE greater than 1\n",
        "POP_SIZE =    2  #@param{type:\"slider\", min:1, max:100}\n",
        "EVOLUTION_FREQUENCY =  100#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Genetic algorithm methods\n",
        "\n",
        "#@markdown **Microbial** - loser of randomly selected pair is replaced by mutated winner. A low selection pressure.\n",
        "\n",
        "#@markdown **Evolutionary Strategies** - mutantions of the best individual replace the rest of the population. Much higher selection pressure than Microbial GA.\n",
        "GA_METHOD = \"Microbial\"  #@param [\"Evolutionary Strategies\", \"Microbial\"]\n",
        "#@markdown ### Mutation levels\n",
        "#@markdown Scale mutation applied to position and rotation, scale, distortio, colour and patch swaps.\n",
        "POS_AND_ROT_MUTATION_SCALE = 0  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "SCALE_MUTATION_SCALE = 0  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "DISTORT_MUTATION_SCALE = 0  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "COLOUR_MUTATION_SCALE = 0  #@param{type:\"slider\", min:0.0, max:0.3, step:0.01}\n",
        "PATCH_MUTATION_PROBABILITY = 1  #@param{type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "#@markdown Limit the number of individuals shown during training\n",
        "MAX_MULTIPLE_VISUALISATIONS =   5#@param {type:\"integer\"}\n",
        "#@markdown Save video of population sample over time.\n",
        "POPULATION_VIDEO = True  #@param (type:\"boolean\")\n",
        "\n",
        "USE_EVOLUTION = POP_SIZE > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "37P41H1vGu-0"
      },
      "outputs": [],
      "source": [
        "# @title Saving images on Drive\n",
        "#@markdown Displayed results can also be stored on Google Drive.\n",
        "STORE_ON_GOOGLE_DRIVE = False  #@param {type:\"boolean\"}\n",
        "GOOGLE_DRIVE_RESULTS_DIR = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "MOUNT_DIR = \"/content/drive\"\n",
        "\n",
        "if STORE_ON_GOOGLE_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount(MOUNT_DIR)\n",
        "  DIR_RESULTS = pathlib.PurePath(MOUNT_DIR, \"MyDrive\", GOOGLE_DRIVE_RESULTS_DIR)\n",
        "  print(f\"Storing results on Google Drive in {DIR_RESULTS}\")\n",
        "else:\n",
        "  DIR_RESULTS = \"/content\"\n",
        "  print(f\"Storing results in Colab in {DIR_RESULTS}\")\n",
        "\n",
        "pathlib.Path(DIR_RESULTS).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbGkH7v-_--H"
      },
      "source": [
        "# Images patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ7_ChjTPcnf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Functions used for loading patches\n",
        "\n",
        "def add_binary_alpha_mask(patch):\n",
        "  \"\"\"Black pixels treated as having alpha=0, all other pixels have alpha=255\"\"\"\n",
        "  shape = patch.shape\n",
        "  mask = ((patch.sum(2) > 0) * 255).astype(np.uint8)\n",
        "  return np.concatenate([patch, np.expand_dims(mask, -1)], axis=-1)\n",
        "\n",
        "\n",
        "def resize_patch(patch, coeff):\n",
        "  return resize(patch.astype(float),\n",
        "                (int(np.round(patch.shape[0] * coeff)),\n",
        "                 int(np.round(patch.shape[1] * coeff))))\n",
        "\n",
        "\n",
        "def print_size_segmented_data(segmented_data):\n",
        "  size_max = 0\n",
        "  shape_max = None\n",
        "  size_min = np.infty\n",
        "  shape_min = None\n",
        "  ws = []\n",
        "  hs = []\n",
        "  for i, segment in enumerate(segmented_data):\n",
        "    segment = segment.swapaxes(0, 1) \n",
        "    shape_i = segment.shape\n",
        "    size_i = shape_i[0] * shape_i[1]\n",
        "    if size_i > size_max:\n",
        "      shape_max = shape_i\n",
        "      size_max = size_i\n",
        "    if size_i < size_min:\n",
        "      shape_min = shape_i\n",
        "      size_min = size_i\n",
        "    im_i = cv2.cvtColor(segment, cv2.COLOR_RGBA2BGRA)\n",
        "    im_bgr = im_i[:, :, :3]\n",
        "    im_mask = np.tile(im_i[:, :, 3:], (1, 1, 3))\n",
        "    im_render = np.concatenate([im_bgr, im_mask], 1)\n",
        "    print(f'Patch {i} of shape {shape_i}')\n",
        "    cv2_imshow(im_render)\n",
        "  print(f\"{len(segmented_data)} patches, max {shape_max}, min {shape_min}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXuH-1hcAl2M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load segmented patches\n",
        "\n",
        "#@markdown Patch files - select example sets or your own:\n",
        "PATCH_SET = \"Animals\" #@param [\"Fruit and veg\", \"Sea glass\", \"Animals\", \"Handwritten MNIST\", \"Upload to Colab\", \"Load from URL\", \"Load from Google Drive\"]\n",
        "#@markdown URL if downloading .npy file from website:\n",
        "URL_TO_PATCH_FILE = \"\" #@param {type:\"string\"}\n",
        "#@markdown Path if loading .npy file from Google Drive:\n",
        "DRIVE_PATH_TO_PATCH_FILE = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def cached_url_download(url):\n",
        "  cache_filename = os.path.basename(url)\n",
        "  cache = pathlib.Path(cache_filename)\n",
        "  if not cache.is_file():\n",
        "    print(\"Downloading \" + cache_filename)\n",
        "    r = requests.get(url)\n",
        "    bytesio_object = io.BytesIO(r.content)\n",
        "    with open(cache_filename, \"wb\") as f:\n",
        "        f.write(bytesio_object.getbuffer())\n",
        "  else:\n",
        "    print(\"Using cached version of \" + cache_filename)\n",
        "  return np.load(cache, allow_pickle=True)\n",
        "\n",
        "def upload_file():\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "    with open(fn, 'rb') as f:\n",
        "      return np.load(f, allow_pickle = True)\n",
        "\n",
        "\n",
        "examples = {\"Fruit and veg\" : \"fruit.npy\", \n",
        "            \"Sea glass\" : \"shore_glass.npy\",\n",
        "            #\"Chrisantha\" : \"chrisantha2.npy\",\n",
        "            \"Handwritten MNIST\" : \"handwritten_mnist.npy\",\n",
        "            \"Animals\" : \"animals.npy\"}\n",
        "\n",
        "if PATCH_SET in examples:\n",
        "  repo_root = \"https://github.com/deepmind/arnheim/raw/main\"\n",
        "  segmented_data_initial = cached_url_download(\n",
        "      f\"{repo_root}/collage_patches/{examples[PATCH_SET]}\")\n",
        "elif PATCH_SET == \"Load from URL\":\n",
        "  segmented_data_initial = cached_url_download(URL_TO_PATCH_FILE)\n",
        "elif PATCH_SET == \"Upload to Colab\":\n",
        "  segmented_data_initial = upload_file()\n",
        "else:  # \"Load from Google Drive\"\n",
        "  drive.mount(MOUNT_DIR)\n",
        "  data_file = pathlib.PurePath(MOUNT_DIR, \"MyDrive\", DRIVE_PATH_TO_PATCH_FILE)\n",
        "  print(\"Reading\", data_file)\n",
        "  segmented_data_initial = np.load(data_file, allow_pickle=True)\n",
        "\n",
        "\n",
        "segmented_data_initial_tmp = []\n",
        "for i in range(len(segmented_data_initial)):\n",
        "  if segmented_data_initial[i].shape[2] == 3:\n",
        "    segmented_data_initial_tmp.append(add_binary_alpha_mask(\n",
        "        segmented_data_initial[i]))\n",
        "  else:\n",
        "    segmented_data_initial_tmp.append(\n",
        "        segmented_data_initial[i])\n",
        "    \n",
        "segmented_data_initial = segmented_data_initial_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg4ed9tyi6vZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Resize image patches to low- and high-res.\n",
        "\n",
        "NORMALIZE_PATCH_BRIGHTNESS = False  #@param {type: \"boolean\"}\n",
        "#@markdown Scale all patches by same amount?\n",
        "FIXED_SCALE_PATCHES = False #@param {type:\"boolean\"}\n",
        "FIXED_SCALE_COEFF =   0.7#@param {type:\"number\"}\n",
        "\n",
        "def normalise_patch_brightness(patch):\n",
        "  max_intensity = max(patch.max(), 1.0)\n",
        "  return ((patch / max_intensity) * 255).astype(np.uint8)\n",
        "\n",
        "# Permute the order of the segmented images. \n",
        "num_patches = len(segmented_data_initial)\n",
        "order = np.random.permutation(num_patches)\n",
        "\n",
        "# Compress all images until they are at most 50% of the large canvas size. \n",
        "height_large_max = CANVAS_HEIGHT * MULTIPLIER_BIG_IMAGE / PATCH_MAX_PROPORTION\n",
        "width_large_max = CANVAS_WIDTH * MULTIPLIER_BIG_IMAGE / PATCH_MAX_PROPORTION\n",
        "print(\n",
        "    f\"Max patch size on large image: ({height_large_max}, {width_large_max})\")\n",
        "segmented_data = []\n",
        "segmented_data_high_res = []\n",
        "for patch_i in range(num_patches):\n",
        "  segmented_data_initial_i = segmented_data_initial[\n",
        "      order[patch_i]].astype(np.float32).swapaxes(0, 1)\n",
        "  shape_i = segmented_data_initial_i.shape\n",
        "  h_i = shape_i[0]\n",
        "  w_i = shape_i[1]\n",
        "  if h_i >= PATCH_HEIGHT_MIN and w_i >= PATCH_WIDTH_MIN:\n",
        "    # Coefficient for resizing the patch.\n",
        "    if FIXED_SCALE_PATCHES:\n",
        "      coeff_i_large = FIXED_SCALE_COEFF\n",
        "    else:\n",
        "      coeff_i_large = 1.0\n",
        "      if h_i > height_large_max:\n",
        "        coeff_i_large = height_large_max / h_i\n",
        "      if w_i > width_large_max:\n",
        "        coeff_i_large = min(coeff_i_large, width_large_max / w_i)\n",
        "\n",
        "    # Resize the high-res patch?\n",
        "    if coeff_i_large < 1.0:\n",
        "      segmented_data_high_res_i = resize_patch(segmented_data_initial_i,\n",
        "                                              coeff_i_large)\n",
        "    else:\n",
        "      segmented_data_high_res_i = np.copy(segmented_data_initial_i)\n",
        "\n",
        "    # Resize the low-res patch.\n",
        "    coeff_i = coeff_i_large / MULTIPLIER_BIG_IMAGE\n",
        "    segmented_data_i = resize_patch(segmented_data_initial_i, coeff_i)\n",
        "    if NORMALIZE_PATCH_BRIGHTNESS:\n",
        "      segmented_data_i[...,:3] = normalise_patch_brightness(\n",
        "          segmented_data_i[...,:3])\n",
        "      segmented_data_high_res_i[...,:3] = normalise_patch_brightness(\n",
        "          segmented_data_high_res_i[...,:3])\n",
        "    segmented_data_high_res_i = segmented_data_high_res_i.astype(np.uint8)\n",
        "    segmented_data_high_res.append(segmented_data_high_res_i)\n",
        "    segmented_data_i = segmented_data_i.astype(np.uint8)\n",
        "    segmented_data.append(segmented_data_i)\n",
        "    print(\"{}/{}: initial {} -> small {}, large {} x{:.2f}\".format(\n",
        "        patch_i, num_patches, shape_i, segmented_data_i.shape, \n",
        "        segmented_data_high_res_i.shape,\n",
        "        coeff_i_large))\n",
        "  else:\n",
        "    print(f\"Discard patch of size {h_i}x{w_i}\")\n",
        "\n",
        "print(\"Patch sizes during optimisation:\")\n",
        "print_size_segmented_data(segmented_data)\n",
        "print(\"Patch sizes for high-resolution final image:\")\n",
        "print_size_segmented_data(segmented_data_high_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWV2a7ZekET"
      },
      "source": [
        "# Colour and affine transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxR0ZFpAebsH"
      },
      "source": [
        "## Affine transform classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "z1JiWgxJas5l"
      },
      "outputs": [],
      "source": [
        "class PopulationAffineTransforms(torch.nn.Module):\n",
        "  \"\"\"Population-based Affine Transform network.\"\"\"\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationAffineTransforms, self).__init__()\n",
        "\n",
        "    self._pop_size = pop_size\n",
        "    matrices_translation = (\n",
        "        np.random.rand(pop_size, num_patches, 2, 1) * (MAX_TRANS - MIN_TRANS) \n",
        "        + MIN_TRANS)\n",
        "    matrices_rotation = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (MAX_ROT - MIN_ROT)\n",
        "        + MIN_ROT)\n",
        "    matrices_scale = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (MAX_SCALE - MIN_SCALE) \n",
        "        + MIN_SCALE)\n",
        "    matrices_squeeze = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (\n",
        "            (MAX_SQUEEZE - MIN_SQUEEZE) + MIN_SQUEEZE))\n",
        "    matrices_shear = (\n",
        "        np.random.rand(pop_size, num_patches, 1, 1) * (MAX_SHEAR - MIN_SHEAR) \n",
        "        + MIN_SHEAR)\n",
        "    self.translation = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_translation, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.rotation = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_rotation, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.scale = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_scale, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.squeeze = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_squeeze, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.shear = torch.nn.Parameter(\n",
        "        torch.tensor(matrices_shear, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._identity = (\n",
        "        torch.ones((pop_size, num_patches, 1, 1)) * torch.eye(2).unsqueeze(0)\n",
        "        ).to(device)\n",
        "    self._zero_column = torch.zeros((pop_size, num_patches, 2, 1)).to(device)\n",
        "    self._unit_row = (\n",
        "        torch.ones((pop_size, num_patches, 1, 1)) * torch.tensor([0., 0., 1.])\n",
        "        ).to(device)\n",
        "    self._zeros = torch.zeros((pop_size, num_patches, 1, 1)).to(device)\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.translation.data = self.translation.data.clamp(\n",
        "        min=MIN_TRANS, max=MAX_TRANS)\n",
        "    self.rotation.data = self.rotation.data.clamp(\n",
        "        min=MIN_ROT, max=MAX_ROT)\n",
        "    self.scale.data = self.scale.data.clamp(\n",
        "        min=MIN_SCALE, max=MAX_SCALE)\n",
        "    self.squeeze.data = self.squeeze.data.clamp(\n",
        "        min=MIN_SQUEEZE, max=MAX_SQUEEZE)\n",
        "    self.shear.data = self.shear.data.clamp(\n",
        "        min=MIN_SHEAR, max=MAX_SHEAR)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    \"\"\"Copy parameters to child, mutating transform parameters.\"\"\"\n",
        "    with torch.no_grad():\n",
        "      self.translation[child, ...] = (self.translation[parent, ...] \n",
        "          + POS_AND_ROT_MUTATION_SCALE * torch.randn(\n",
        "              self.translation[child, ...].shape).to(device))\n",
        "      self.rotation[child, ...] = (self.rotation[parent, ...]  \n",
        "          + POS_AND_ROT_MUTATION_SCALE * torch.randn(\n",
        "              self.rotation[child, ...].shape).to(device))\n",
        "      self.scale[child, ...] = (self.scale[parent, ...] \n",
        "          + SCALE_MUTATION_SCALE * torch.randn(\n",
        "              self.scale[child, ...].shape).to(device))\n",
        "      self.squeeze[child, ...] = (self.squeeze[parent, ...]\n",
        "          + DISTORT_MUTATION_SCALE * torch.randn(\n",
        "              self.squeeze[child, ...].shape).to(device))\n",
        "      self.shear[child, ...] = (self.shear[parent, ...]\n",
        "          + DISTORT_MUTATION_SCALE * torch.randn(\n",
        "              self.shear[child, ...].shape).to(device))\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other spatial transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.translation[idx_to, ...] = other.translation[idx_from, ...]\n",
        "      self.rotation[idx_to, ...] = other.rotation[idx_from, ...]\n",
        "      self.scale[idx_to, ...] = other.scale[idx_from, ...]\n",
        "      self.squeeze[idx_to, ...] = other.squeeze[idx_from, ...]\n",
        "      self.shear[idx_to, ...] = other.shear[idx_from, ...]\n",
        "\n",
        "  def forward(self, x):\n",
        "    self._clamp()\n",
        "    scale_affine_mat = torch.cat([\n",
        "        torch.cat([self.scale, self.shear], 3),\n",
        "        torch.cat([self._zeros, self.scale * self.squeeze], 3)],\n",
        "        2)\n",
        "    scale_affine_mat = torch.cat([\n",
        "        torch.cat([scale_affine_mat, self._zero_column], 3),\n",
        "        self._unit_row], 2)\n",
        "    rotation_affine_mat = torch.cat([\n",
        "        torch.cat([torch.cos(self.rotation), -torch.sin(self.rotation)], 3),\n",
        "        torch.cat([torch.sin(self.rotation), torch.cos(self.rotation)], 3)],\n",
        "        2)\n",
        "    rotation_affine_mat = torch.cat([\n",
        "        torch.cat([rotation_affine_mat, self._zero_column], 3),\n",
        "        self._unit_row], 2)\n",
        "    \n",
        "    scale_rotation_mat = torch.matmul(scale_affine_mat,\n",
        "                                      rotation_affine_mat)[:, :, :2, :]\n",
        "    # Population and patch dimensions (0 and 1) need to be merged.\n",
        "    # E.g. from (POP_SIZE, NUM_PATCHES, CHANNELS, WIDTH, HEIGHT) \n",
        "    # to (POP_SIZE * NUM_PATCHES, CHANNELS, WIDTH, HEIGHT)\n",
        "    scale_rotation_mat = scale_rotation_mat[:, :, :2, :].view(\n",
        "        1, -1, *(scale_rotation_mat[:, :, :2, :].size()[2:])).squeeze()\n",
        "    x = x.view(1, -1, *(x.size()[2:])).squeeze()\n",
        "    scaled_rotated_grid = F.affine_grid(\n",
        "        scale_rotation_mat, x.size(), align_corners=True)\n",
        "    scaled_rotated_x = F.grid_sample(x, scaled_rotated_grid, align_corners=True)\n",
        "\n",
        "    translation_affine_mat = torch.cat([self._identity, self.translation], 3)\n",
        "    translation_affine_mat = translation_affine_mat.view(\n",
        "        1, -1, *(translation_affine_mat.size()[2:])).squeeze()\n",
        "    translated_grid = F.affine_grid(\n",
        "        translation_affine_mat, x.size(), align_corners=True)\n",
        "    y = F.grid_sample(scaled_rotated_x, translated_grid, align_corners=True)\n",
        "    return y.view(self._pop_size, NUM_PATCHES, *(y.size()[1:]))\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.translation = self.translation.to(device)\n",
        "    self.rotation = self.rotation.to(device)\n",
        "    self.scale = self.scale.to(device)\n",
        "    self.squeeze = self.squeeze.to(device)\n",
        "    self.shear = self.shear.to(device)\n",
        "    self._identity = self._identity.to(device)\n",
        "    self._zero_column = self._zero_column.to(device)\n",
        "    self._unit_row = self._unit_row.to(device)\n",
        "    self._zeros = self._zeros.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9GsjxMcgAP7"
      },
      "source": [
        "## RGB and HSV color transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRc63pIMNwtc"
      },
      "outputs": [],
      "source": [
        "class PopulationOrderOnlyTransforms(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationOrderOnlyTransforms, self).__init__()\n",
        "\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    population_zeros = np.ones((pop_size, num_patches, 1, 1, 1))\n",
        "    population_orders = np.random.rand(pop_size, num_patches, 1, 1, 1)\n",
        "\n",
        "    self._zeros = torch.nn.Parameter(\n",
        "        torch.tensor(population_zeros, dtype=torch.float),\n",
        "        requires_grad=False)\n",
        "    self.orders = torch.nn.Parameter(\n",
        "        torch.tensor(population_orders, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._hsv_to_rgb = hsv.HsvToRgb()\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.orders.data = self.orders.data.clamp(min=0.0, max=1.0)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      self.orders[child, ...] = self.orders[parent, ...]\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other colour transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.orders[idx_to, ...] = other.orders[idx_from, ...]\n",
        "\n",
        "  def forward(self, x):\n",
        "    self._clamp()\n",
        "    colours = torch.cat(\n",
        "        [self._zeros, self._zeros, self._zeros, self._zeros, self.orders],\n",
        "        2)\n",
        "    return colours * x\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.orders = self.orders.to(device)\n",
        "    self._zeros = self._zeros.to(device)\n",
        "\n",
        "\n",
        "class PopulationColourHSVTransforms(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationColourHSVTransforms, self).__init__()\n",
        "\n",
        "    print('PopulationColourHSVTransforms for {} patches, {} individuals'.format(\n",
        "        num_patches, pop_size))\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    coeff_hue = 0.5 * (MAX_HUE - MIN_HUE) + MIN_HUE\n",
        "    coeff_sat = 0.5 * (MAX_SAT - MIN_SAT) + MIN_SAT\n",
        "    coeff_val = 0.5 * (MAX_VAL - MIN_VAL) + MIN_VAL\n",
        "    population_hues = np.random.rand(pop_size, num_patches, 1, 1, 1) * coeff_hue\n",
        "    population_saturations = np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * coeff_sat\n",
        "    population_values = np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * coeff_val\n",
        "    population_zeros = np.ones((pop_size, num_patches, 1, 1, 1))\n",
        "    population_orders = np.random.rand(pop_size, num_patches, 1, 1, 1)\n",
        "    \n",
        "    self.hues = torch.nn.Parameter(\n",
        "        torch.tensor(population_hues, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.saturations = torch.nn.Parameter(\n",
        "        torch.tensor(population_saturations, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.values = torch.nn.Parameter(\n",
        "        torch.tensor(population_values, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._zeros = torch.nn.Parameter(\n",
        "        torch.tensor(population_zeros, dtype=torch.float),\n",
        "        requires_grad=False)\n",
        "    self.orders = torch.nn.Parameter(\n",
        "        torch.tensor(population_orders, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._hsv_to_rgb = hsv.HsvToRgb()\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.hues.data = self.hues.data.clamp(min=MIN_HUE, max=MAX_HUE)\n",
        "    self.saturations.data = self.saturations.data.clamp(\n",
        "        min=MIN_SAT, max=MAX_SAT)\n",
        "    self.values.data = self.values.data.clamp(min=MIN_VAL, max=MAX_VAL)\n",
        "    self.orders.data = self.orders.data.clamp(min=0.0, max=1.0)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      self.hues[child, ...] = (\n",
        "          self.hues[parent, ...]\n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.hues[child, ...].shape).to(device))\n",
        "      self.saturations[child, ...] = (\n",
        "          self.saturations[parent, ...]\n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.saturations[child, ...].shape).to(device))\n",
        "      self.values[child, ...] = (\n",
        "          self.values[parent, ...]\n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.values[child, ...].shape).to(device))\n",
        "      self.orders[child, ...] = self.orders[parent, ...]\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other colour transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.hues[idx_to, ...] = other.hues[idx_from, ...]\n",
        "      self.saturations[idx_to, ...] = other.saturations[idx_from, ...]\n",
        "      self.values[idx_to, ...] = other.values[idx_from, ...]\n",
        "      self.orders[idx_to, ...] = other.orders[idx_from, ...]\n",
        "\n",
        "  def forward(self, image):\n",
        "    self._clamp()\n",
        "    colours = torch.cat(\n",
        "        [self.hues, self.saturations, self.values, self._zeros, self.orders], 2)\n",
        "    hsv_image = colours * image\n",
        "    rgb_image = self._hsv_to_rgb(hsv_image[:, :, :3, :, :])\n",
        "    return torch.cat([rgb_image, hsv_image[:, :, 3:, :, :]], axis=2)\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.hues = self.hues.to(device)\n",
        "    self.saturations = self.saturations.to(device)\n",
        "    self.values = self.values.to(device)\n",
        "    self.orders = self.orders.to(device)\n",
        "    self._zeros = self._zeros.to(device)\n",
        "\n",
        "\n",
        "class PopulationColourRGBTransforms(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, num_patches=1, pop_size=1):\n",
        "    super(PopulationColourRGBTransforms, self).__init__()\n",
        "\n",
        "    print('PopulationColourRGBTransforms for {} patches, {} individuals'.format(\n",
        "        num_patches, pop_size))\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    rgb_init_range = INITIAL_MAX_RGB - INITIAL_MIN_RGB\n",
        "    population_reds = (np.random.rand(pop_size, num_patches, 1, 1, 1) \n",
        "        * rgb_init_range) + INITIAL_MIN_RGB\n",
        "    population_greens = (np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * rgb_init_range) + INITIAL_MIN_RGB\n",
        "    population_blues = (np.random.rand(\n",
        "        pop_size, num_patches, 1, 1, 1) * rgb_init_range) + INITIAL_MIN_RGB\n",
        "    population_zeros = np.ones((pop_size, num_patches, 1, 1, 1))\n",
        "    population_orders = np.random.rand(pop_size, num_patches, 1, 1, 1)\n",
        "\n",
        "    self.reds = torch.nn.Parameter(\n",
        "        torch.tensor(population_reds, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.greens = torch.nn.Parameter(\n",
        "        torch.tensor(population_greens, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self.blues = torch.nn.Parameter(\n",
        "        torch.tensor(population_blues, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "    self._zeros = torch.nn.Parameter(\n",
        "        torch.tensor(population_zeros, dtype=torch.float),\n",
        "        requires_grad=False)\n",
        "    self.orders = torch.nn.Parameter(\n",
        "        torch.tensor(population_orders, dtype=torch.float),\n",
        "        requires_grad=True)\n",
        "\n",
        "  def _clamp(self):\n",
        "    self.reds.data = self.reds.data.clamp(min=MIN_RGB, max=MAX_RGB)\n",
        "    self.greens.data = self.greens.data.clamp(min=MIN_RGB, max=MAX_RGB)\n",
        "    self.blues.data = self.blues.data.clamp(min=MIN_RGB, max=MAX_RGB)\n",
        "    self.orders.data = self.orders.data.clamp(min=0.0, max=1.0)\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      self.reds[child, ...] = (\n",
        "          self.reds[parent, ...] \n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.reds[child, ...].shape).to(device))\n",
        "      self.greens[child, ...] = (\n",
        "          self.greens[parent, ...] \n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.greens[child, ...].shape).to(device))\n",
        "      self.blues[child, ...] = (\n",
        "          self.blues[parent, ...] \n",
        "          + COLOUR_MUTATION_SCALE * torch.randn(\n",
        "              self.blues[child, ...].shape).to(device))\n",
        "      self.orders[child, ...] = self.orders[parent, ...] \n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other colour transform, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.reds[idx_to, ...] = other.reds[idx_from, ...]\n",
        "      self.greens[idx_to, ...] = other.greens[idx_from, ...]\n",
        "      self.blues[idx_to, ...] = other.blues[idx_from, ...]\n",
        "      self.orders[idx_to, ...] = other.orders[idx_from, ...]\n",
        "\n",
        "  def forward(self, x):\n",
        "    self._clamp()\n",
        "    colours = torch.cat(\n",
        "        [self.reds, self.greens, self.blues, self._zeros, self.orders], 2)\n",
        "    return colours * x\n",
        "\n",
        "  def tensor_to(self, device):\n",
        "    self.reds = self.reds.to(device)\n",
        "    self.greens = self.greens.to(device)\n",
        "    self.blues = self.blues.to(device)\n",
        "    self.orders = self.orders.to(device)\n",
        "    self._zeros = self._zeros.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubwSr59fgk0C"
      },
      "source": [
        "# Rendering functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "GCro-8edu6WX"
      },
      "outputs": [],
      "source": [
        "RENDER_EPSILON = 1e-8\n",
        "RENDER_OVERLAP_TEMPERATURE = 0.1\n",
        "RENDER_OVERLAP_ZERO_OFFSET = -5\n",
        "RENDER_OVERLAP_MASK_THRESHOLD = 0.5\n",
        "RENDER_TRANSPARENCY_MASK_THRESHOLD = 0.1\n",
        "\n",
        "\n",
        "def population_render_transparency(x, b=None):\n",
        "  \"\"\"Image rendering function that renders all patches on top of one another,\n",
        "     with transparency, using black as the transparent colour.\n",
        "\n",
        "  Args:\n",
        "    x: tensor of transformed RGB image patches of shape [S, B, 5, H, W].\n",
        "    b: optional tensor of background RGB image of shape [S, 3, H, W].\n",
        "  Returns:\n",
        "    Tensor of rendered RGB images of shape [S, 3, H, W].\n",
        "  \"\"\"\n",
        "  # Sum the RGB patches [S, B, 3, H, W] as [S, 3, H, W].\n",
        "  y = x[:, :, :3, :, :].sum(1)\n",
        "  if INVERT_COLOURS:\n",
        "    y[:, :3, :, :] = 1.0 - y[:, :3, :, :]\n",
        "  # Add backgrounds [S, 3, H, W].\n",
        "  if b is not None:\n",
        "    b = b.cuda() if x.is_cuda else b.cpu()\n",
        "    y = torch.where(y.sum(1, keepdim=True) > RENDER_TRANSPARENCY_MASK_THRESHOLD,\n",
        "                    y[:, :3, :, :], b.unsqueeze(0)[:, :3, :, :])\n",
        "  return y.clamp(0., 1.).permute(0, 2, 3, 1)\n",
        "\n",
        "\n",
        "def population_render_masked_transparency(x, b=None):\n",
        "  \"\"\"Image rendering function that renders all patches on top of one another,\n",
        "     with transparency, using the alpha chanel as the mask colour.\n",
        "\n",
        "  Args:\n",
        "    x: tensor of transformed RGB image patches of shape [S, B, 5, H, W].\n",
        "    b: optional tensor of background RGB image of shape [S, 3, H, W].\n",
        "  Returns:\n",
        "    Tensor of rendered RGB images of shape [S, 3, H, W].\n",
        "  \"\"\"\n",
        "  # Get the patch mask [S, B, 1, H, W] and sum of masks [S, 1, H, W].\n",
        "  mask = x[:, :, 3:4, :, :]\n",
        "  mask_sum = mask.sum(1) + RENDER_EPSILON\n",
        "  # Mask the RGB patches [S, B, 4, H, W] -> [S, B, 3, H, W].\n",
        "  masked_x = x[:, :, :3, :, :] * mask\n",
        "  # Compute mean of the RGB patches [S, B, 3, H, W] as [S, 3, H, W].\n",
        "  x_sum = masked_x.sum(1)\n",
        "  y = torch.where(\n",
        "      mask_sum > RENDER_EPSILON, x_sum / mask_sum, mask_sum)\n",
        "  # Anti-aliasing on the countours of the sum of patches.\n",
        "  y = y * mask_sum.clamp(0., 1.)\n",
        "  if INVERT_COLOURS:\n",
        "    y[:, :3, :, :] = 1.0 - y[:, :3, :, :]\n",
        "  # Add backgrounds [S, 3, H, W].\n",
        "  if b is not None:\n",
        "    b = b.cuda() if x.is_cuda else b.cpu()\n",
        "    y = torch.where(mask.sum(1) > RENDER_OVERLAP_MASK_THRESHOLD, y[:, :3, :, :],\n",
        "                  b.unsqueeze(0)[:, :3, :, :])\n",
        "  return y.clamp(0., 1.).permute(0, 2, 3, 1)\n",
        "\n",
        "\n",
        "def population_render_overlap(x, b=None, gamma=None):\n",
        "  \"\"\"Image rendering function that overlays all patches on top of one another,\n",
        "     with semi-translucent overlap, using the alpha chanel as the mask colour\n",
        "     and the 5th channel as the order for the overlapped images.\n",
        "\n",
        "  Args:\n",
        "    x: tensor of transformed RGB image patches of shape [S, B, 5, H, W].\n",
        "    b: optional tensor of background RGB image of shape [S, 3, H, W].\n",
        "  Returns:\n",
        "    Tensor of rendered RGB images of shape [S, 3, H, W].\n",
        "  \"\"\"\n",
        "  # Get the patch mask [S, B, 1, H, W].\n",
        "  mask = x[:, :, 3:4, :, :]\n",
        "  # Mask the patches [S, B, 4, H, W] -> [S, B, 3, H, W]\n",
        "  masked_x = x[:, :, :3, :, :] * mask * mask\n",
        "  # Mask the orders [S, B, 1, H, W] -> [S, B, 1, H, W]\n",
        "  order = torch.where(\n",
        "      mask > RENDER_OVERLAP_MASK_THRESHOLD,\n",
        "      x[:, :, 4:, :, :] * mask / RENDER_OVERLAP_TEMPERATURE,\n",
        "      mask + RENDER_OVERLAP_ZERO_OFFSET)\n",
        "  # Get weights from orders [S, B, 1, H, W]\n",
        "  weights = F.softmax(order, dim=1)\n",
        "  # Apply weights to masked patches and compute mean over patches [S, 3, H, W].\n",
        "  y = (weights * masked_x).sum(1)\n",
        "  if INVERT_COLOURS:\n",
        "    y[:, :3, :, :] = 1.0 - y[:, :3, :, :]\n",
        "  if b is not None:\n",
        "    b = b.cuda() if x.is_cuda else b.cpu()\n",
        "    y = torch.where(mask.sum(1) > RENDER_OVERLAP_MASK_THRESHOLD, y[:, :3, :, :],\n",
        "                  b.unsqueeze(0)[:, :3, :, :])\n",
        "  return y.clamp(0., 1.).permute(0, 2, 3, 1)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ckYmVuAAO7x"
      },
      "source": [
        "# Collage network definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "zrReSIFc6tOq"
      },
      "outputs": [],
      "source": [
        "class PopulationCollage(torch.nn.Module):\n",
        "  \"\"\"Population-based segmentation collage network.\"\"\"\n",
        "  def __init__(self,\n",
        "               pop_size=1,\n",
        "               is_high_res=False,\n",
        "               segmented_data=None,\n",
        "               background_image=None):\n",
        "    \"\"\"Constructor, relying on global parameters.\"\"\"\n",
        "    super(PopulationCollage, self).__init__()\n",
        "\n",
        "    # Population size.\n",
        "    self._pop_size = pop_size\n",
        "\n",
        "    # Create the spatial transformer and colour transformer for patches.\n",
        "    self.spatial_transformer = PopulationAffineTransforms(\n",
        "        num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "    if COLOUR_TRANSFORMATIONS == \"HSV space\":\n",
        "      self.colour_transformer = PopulationColourHSVTransforms(\n",
        "          num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "    elif COLOUR_TRANSFORMATIONS == \"RGB space\":\n",
        "      self.colour_transformer = PopulationColourRGBTransforms(\n",
        "          num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "    else:\n",
        "      self.colour_transformer = PopulationOrderOnlyTransforms(\n",
        "          num_patches=NUM_PATCHES, pop_size=pop_size).cuda()\n",
        "\n",
        "    # Optimisation is run in low-res, final rendering is in high-res.\n",
        "    self._high_res = is_high_res\n",
        "\n",
        "    # Store the background image (low- and high-res).\n",
        "    self._background_image = background_image\n",
        "    if self._background_image is not None:\n",
        "      print(f'Background image of size {self._background_image.shape}')\n",
        "\n",
        "    # Store the dataset (low- and high-res).\n",
        "    self._dataset = segmented_data\n",
        "    #print(f'There are {len(self._dataset)} image patches in the dataset')\n",
        "\n",
        "    # Initial set of indices, pointing to the NUM_PATCHES first dataset images. \n",
        "    self.patch_indices = [np.arange(NUM_PATCHES) % len(self._dataset)\n",
        "                          for _ in range(pop_size)]\n",
        "\n",
        "    # Patches in low and high-res.\n",
        "    self.patches = None\n",
        "    self.store_patches()\n",
        "\n",
        "  def store_patches(self, population_idx=None):\n",
        "    \"\"\"Store the image patches for each population element.\"\"\"\n",
        "    t0 = time.time()\n",
        "\n",
        "    if population_idx is not None and self.patches is not None:\n",
        "      list_indices = [population_idx]\n",
        "      #print(f'Reload {NUM_PATCHES} image patches for [{population_idx}]')\n",
        "      self.patches[population_idx, :, :4, :, :] = 0\n",
        "    else:\n",
        "      list_indices = np.arange(self._pop_size)\n",
        "      #print(f'Store {NUM_PATCHES} image patches for [1, ..., {self._pop_size}]')\n",
        "      if self._high_res:\n",
        "        self.patches = torch.zeros(\n",
        "            1, NUM_PATCHES, 5, CANVAS_HEIGHT * MULTIPLIER_BIG_IMAGE,\n",
        "            CANVAS_WIDTH * MULTIPLIER_BIG_IMAGE).to('cpu')\n",
        "      else:\n",
        "        self.patches = torch.zeros(\n",
        "            self._pop_size, NUM_PATCHES, 5, CANVAS_HEIGHT, CANVAS_WIDTH\n",
        "            ).to(device)\n",
        "      self.patches[:, :, 4, :, :] = 1.0\n",
        "\n",
        "    # Put the segmented data into the patches.\n",
        "    for i in list_indices:\n",
        "      for j in range(NUM_PATCHES):\n",
        "        k = self.patch_indices[i][j]\n",
        "        patch_j = torch.tensor(\n",
        "            self._dataset[k].swapaxes(0, 2) / 255.0).to(device)\n",
        "        width_j = patch_j.shape[1]\n",
        "        height_j = patch_j.shape[2]\n",
        "        if self._high_res:\n",
        "          w0 = int((CANVAS_WIDTH * MULTIPLIER_BIG_IMAGE - width_j) / 2.0)\n",
        "          h0 = int((CANVAS_HEIGHT * MULTIPLIER_BIG_IMAGE - height_j) / 2.0)\n",
        "        else:\n",
        "          w0 = int((CANVAS_WIDTH - width_j) / 2.0)\n",
        "          h0 = int((CANVAS_HEIGHT - height_j) / 2.0)\n",
        "        self.patches[i, j, :4, w0:(w0 + width_j), h0:(h0 + height_j)] = patch_j\n",
        "    t1 = time.time()\n",
        "    #print('Updated patches in {:.3f}s'.format(t1-t0))\n",
        "\n",
        "  def copy_and_mutate_s(self, parent, child):\n",
        "    with torch.no_grad():\n",
        "      # Copy the patches indices from the parent to the child.\n",
        "      self.patch_indices[child] = copy.deepcopy(self.patch_indices[parent])\n",
        "      \n",
        "      # Mutate the child patches with a single swap from the original dataset. \n",
        "      if PATCH_MUTATION_PROBABILITY > np.random.uniform():\n",
        "        idx_dataset  = np.random.randint(len(self._dataset))\n",
        "        idx_patch  = np.random.randint(NUM_PATCHES)\n",
        "        self.patch_indices[child][idx_patch] = idx_dataset\n",
        "\n",
        "      # Update all the patches for the child.\n",
        "      self.store_patches(child)\n",
        "  \n",
        "      self.spatial_transformer.copy_and_mutate_s(parent, child)\n",
        "      self.colour_transformer.copy_and_mutate_s(parent, child)\n",
        "\n",
        "  def copy_from(self, other, idx_to, idx_from):\n",
        "    \"\"\"Copy parameters from other collage generator, for selected indices.\"\"\"\n",
        "    assert idx_to < self._pop_size\n",
        "    with torch.no_grad():\n",
        "      self.patch_indices[idx_to] = copy.deepcopy(other.patch_indices[idx_from])\n",
        "      self.store_patches(idx_to)\n",
        "      self.spatial_transformer.copy_from(\n",
        "          other.spatial_transformer, idx_to, idx_from)\n",
        "      self.colour_transformer.copy_from(\n",
        "          other.colour_transformer, idx_to, idx_from)\n",
        "\n",
        "  def forward(self, params=None):\n",
        "    \"\"\"Input-less forward function.\"\"\"\n",
        "\n",
        "    shifted_patches = self.spatial_transformer(self.patches)\n",
        "    background_image = self._background_image\n",
        "    if USE_BACKGROUND == False:\n",
        "      background_image = None\n",
        "    coloured_patches = self.colour_transformer(shifted_patches)\n",
        "    if RENDER_METHOD == \"transparency\":\n",
        "      img = population_render_transparency(coloured_patches, background_image)\n",
        "    elif RENDER_METHOD == \"masked_transparency\":\n",
        "      img = population_render_masked_transparency(\n",
        "          coloured_patches, background_image)\n",
        "    elif RENDER_METHOD == \"opacity\":\n",
        "      if params is not None and 'gamma' in params:\n",
        "        gamma = params['gamma']\n",
        "      else:\n",
        "        gamma = None\n",
        "      img = population_render_overlap(\n",
        "          coloured_patches, background_image)\n",
        "    else:\n",
        "      print(\"Unhandled render method\")\n",
        "    return img\n",
        "\n",
        "  def tensors_to(self, device):\n",
        "    self.spatial_transformer.tensor_to(device)\n",
        "    self.colour_transformer.tensor_to(device)\n",
        "    self.patches = self.patches.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIXzueO3PB-4"
      },
      "source": [
        "# Image and video function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "urXU9M5hS6H0"
      },
      "outputs": [],
      "source": [
        "#@title Image rendering and display\n",
        "\n",
        "def layout_img_batch(img_batch, max_display=None):\n",
        "    # img_batch.shape = (7, 224, 224, 3)  S, H, W, C\n",
        "    img_np = img_batch.transpose(0, 2, 1, 3).clip(0.0, 1.0)  # S, W, H, C\n",
        "    if max_display:\n",
        "      img_np = img_np[:max_display, ...]\n",
        "    sp = img_np.shape\n",
        "    img_np[:, 0, :, :] = 1.0  # White line separator \n",
        "    img_stitch = np.reshape(img_np, (sp[1] * sp[0], sp[2], sp[3]))\n",
        "    img_r = img_stitch.transpose(1, 0, 2)   # H, W, C\n",
        "    return img_r\n",
        "\n",
        "def show_and_save(img_batch, t=None,\n",
        "                  max_display=1, interpolation=\"None\", stitch=True,\n",
        "                  img_format=\"SCHW\", show=True):\n",
        "  \"\"\"Display image.\n",
        "\n",
        "  Args:\n",
        "  \n",
        "    img: image to display\n",
        "    t: time step\n",
        "    max_display: max number of images to display from population\n",
        "    interpolation: interpolate enlarged images\n",
        "    stitch: append images side-by-side\n",
        "    img_format: SHWC or SCHW (the latter used by CLIP)\n",
        "\n",
        "  Returns:\n",
        "    stitched image or None\n",
        "  \"\"\"\n",
        "\n",
        "  if isinstance(img_batch, torch.Tensor):\n",
        "    img_np = img_batch.detach().cpu().numpy()\n",
        "  else:\n",
        "    img_np = img_batch\n",
        "\n",
        "  if not stitch:\n",
        "    print(f\"image (not stitch) min {img_np.min()}, max {img_np.max()}\")\n",
        "    for i in range(min(max_display, img_np.shape[0])):\n",
        "      img = img_np[i]\n",
        "      if img_format == \"SCHW\":  # Convert to SHWC\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "      img = np.clip(img, 0.0, 1.0)\n",
        "      cv2_imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB) * 255)\n",
        "      \n",
        "      type_image = \"image_\"\n",
        "      if img.shape[1] > CANVAS_WIDTH:\n",
        "        type_image = \"highres_\" + type_image \n",
        "      path_fig = f\"{DIR_RESULTS}/{type_image}{PROMPT}_{str(i)}\"\n",
        "      if t is not None:\n",
        "        path_fig += \"_t_\" + str(t)\n",
        "      path_fig += \".png\"\n",
        "      if show:\n",
        "        cv2_imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB) * 255)\n",
        "      cv2.imwrite(path_fig, img)\n",
        "    return None\n",
        "  else:\n",
        "    print(f\"image (stitch) min {img_np.min()}, max {img_np.max()}\")\n",
        "    img_np = np.clip(img_np, 0.0, 1.0)\n",
        "    num_images = img_np.shape[0]\n",
        "    if img_format == \"SCHW\":  # Convert to SHWC\n",
        "      img_np = img_np.transpose((0, 2, 3, 1))\n",
        "    laid_out = layout_img_batch(img_np, max_display)\n",
        "    if show:\n",
        "      cv2_imshow(cv2.cvtColor(laid_out, cv2.COLOR_BGR2RGB) * 255)\n",
        "    return laid_out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvMiX61xGEOS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Video creator {vertical-output: true}\n",
        "\n",
        "class VideoWriter:\n",
        "  \"\"\"Create a video from image frames.\"\"\"\n",
        "\n",
        "  def __init__(self, filename=\"_autoplay.mp4\", fps=20.0, **kw):\n",
        "    \"\"\"Video creator.\n",
        "\n",
        "    Creates and display a video made from frames. The default\n",
        "    filename causes the video to be displayed on exit.\n",
        "\n",
        "    Args:\n",
        "      filename: name of video file\n",
        "      fps: frames per second for video\n",
        "      **kw: args to be passed to FFMPEG_VideoWriter\n",
        "\n",
        "    Returns:\n",
        "      VideoWriter instance.\n",
        "    \"\"\"\n",
        "\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kw)\n",
        "\n",
        "  def add(self, img):\n",
        "    \"\"\"Add image to video.\n",
        "\n",
        "    Add new frame to image file, creating VideoWriter if requried.\n",
        "\n",
        "    Args:\n",
        "      img: array-like frame, shape [X, Y, 3] or [X, Y]\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in [np.float32, np.float64]:\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer:\n",
        "      self.writer.close()\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *kw):\n",
        "    self.close()\n",
        "    if self.params[\"filename\"] == \"_autoplay.mp4\":\n",
        "      self.show()\n",
        "\n",
        "  def show(self, **kw):\n",
        "    \"\"\"Display video.\n",
        "\n",
        "    Args:\n",
        "      **kw: args to be passed to mvp.ipython_display\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    self.close()\n",
        "    fn = self.params[\"filename\"]\n",
        "    display(mvp.ipython_display(fn, **kw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pUbeONI1d8z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Metadata export\n",
        "\n",
        "def export_metadata(metadata_filename):\n",
        "  metadata = {}\n",
        "  metadata_canvas = {\"CANVAS_WIDTH\": CANVAS_WIDTH,\n",
        "                    \"CANVAS_HEIGHT\": CANVAS_HEIGHT,\n",
        "                    \"MULTIPLIER_BIG_IMAGE\": MULTIPLIER_BIG_IMAGE,\n",
        "                    \"PATCH_WIDTH_MIN\": PATCH_WIDTH_MIN,\n",
        "                    \"PATCH_HEIGHT_MIN\": PATCH_HEIGHT_MIN,\n",
        "                    \"PATCH_MAX_PROPORTION\": PATCH_MAX_PROPORTION}\n",
        "  metadata = {\"canvas\": metadata_canvas}\n",
        "  metadata_collage = {\"RENDER_METHOD\": RENDER_METHOD,\n",
        "                      \"NUM_PATCHES\": NUM_PATCHES,\n",
        "                      \"COLOUR_TRANSFORMATIONS\": COLOUR_TRANSFORMATIONS,\n",
        "                      \"INVERT_COLOURS\": INVERT_COLOURS}\n",
        "  metadata[\"collage\"] = metadata_collage\n",
        "  metadata_affine = {\"MIN_TRANS\": MIN_TRANS,\n",
        "                    \"MAX_TRANS\": MAX_TRANS,\n",
        "                    \"MIN_SCALE\": MIN_SCALE,\n",
        "                    \"MAX_SCALE\": MAX_SCALE,\n",
        "                    \"MIN_SQUEEZE\": MIN_SQUEEZE,\n",
        "                    \"MAX_SQUEEZE\": MAX_SQUEEZE,\n",
        "                    \"MIN_SHEAR\": MIN_SHEAR,\n",
        "                    \"MAX_SHEAR\": MAX_SHEAR,\n",
        "                    \"MIN_ROT_DEG\": MIN_ROT_DEG,\n",
        "                    \"MAX_ROT_DEG\": MAX_ROT_DEG,\n",
        "                    \"MIN_ROT\": MIN_ROT,\n",
        "                    \"MAX_ROT\": MAX_ROT}\n",
        "  metadata[\"affine\"] = metadata_affine\n",
        "  metadata_colour = {\"MIN_RGB\": MIN_RGB,\n",
        "                    \"MAX_RGB\": MAX_RGB,\n",
        "                    \"MIN_HUE\": MIN_HUE,\n",
        "                    \"MAX_HUE_DEG\": MAX_HUE_DEG,\n",
        "                    \"MAX_HUE\": MAX_HUE,\n",
        "                    \"MIN_SAT\": MIN_SAT,\n",
        "                    \"MAX_SAT\": MAX_SAT,\n",
        "                    \"MIN_VAL\": MIN_VAL,\n",
        "                    \"MAX_VAL\": MAX_VAL}\n",
        "  metadata[\"colour\"] = metadata_colour\n",
        "  metadata_training = {\"OPTIM_STEPS\": OPTIM_STEPS,\n",
        "                      \"LEARNING_RATE\": LEARNING_RATE,\n",
        "                      \"USE_IMAGE_AUGMENTATIONS\": USE_IMAGE_AUGMENTATIONS,\n",
        "                      \"NUM_AUGS\": NUM_AUGS,\n",
        "                      \"USE_NORMALIZED_CLIP\": USE_NORMALIZED_CLIP,\n",
        "                      \"GRADIENT_CLIPPING\": GRADIENT_CLIPPING,\n",
        "                      \"INITIAL_SEARCH_SIZE\": INITIAL_SEARCH_SIZE}\n",
        "  metadata[\"training\"] = metadata_training\n",
        "  metadata_evolution = {\"POP_SIZE\": POP_SIZE,\n",
        "                        \"EVOLUTION_FREQUENCY\": EVOLUTION_FREQUENCY,\n",
        "                        \"GA_METHOD\": GA_METHOD,\n",
        "                        \"POS_AND_ROT_MUTATION_SCALE\": POS_AND_ROT_MUTATION_SCALE,\n",
        "                        \"SCALE_MUTATION_SCALE\": SCALE_MUTATION_SCALE,\n",
        "                        \"DISTORT_MUTATION_SCALE\": DISTORT_MUTATION_SCALE,\n",
        "                        \"COLOUR_MUTATION_SCALE\": COLOUR_MUTATION_SCALE,\n",
        "                        \"PATCH_MUTATION_PROBABILITY\": PATCH_MUTATION_PROBABILITY,\n",
        "                        \"MAX_MULTIPLE_VISUALISATIONS\": MAX_MULTIPLE_VISUALISATIONS,\n",
        "                        \"USE_EVOLUTION\": USE_EVOLUTION}\n",
        "  metadata[\"evolution\"] = metadata_evolution\n",
        "  metadata_drive = {\"STORE_ON_GOOGLE_DRIVE\": STORE_ON_GOOGLE_DRIVE,\n",
        "                    \"GOOGLE_DRIVE_RESULTS_DIR\": GOOGLE_DRIVE_RESULTS_DIR}\n",
        "  metadata[\"drive\"] = metadata_drive\n",
        "  metadata_patches = {\"PATCH_SET\": PATCH_SET,\n",
        "                      \"URL_TO_PATCH_FILE\": URL_TO_PATCH_FILE,\n",
        "                      \"DRIVE_PATH_TO_PATCH_FILE\": DRIVE_PATH_TO_PATCH_FILE,\n",
        "                      \"NORMALIZE_PATCH_BRIGHTNESS\": NORMALIZE_PATCH_BRIGHTNESS,\n",
        "                      \"FIXED_SCALE_PATCHES\": FIXED_SCALE_PATCHES,\n",
        "                      \"FIXED_SCALE_COEFF\": FIXED_SCALE_COEFF}\n",
        "  metadata[\"patches\"] = metadata_patches\n",
        "\n",
        "  metadata_prompt = {\"PROMPT\": PROMPT,\n",
        "                    \"FILE_BASENAME\": FILE_BASENAME,\n",
        "                    \"USE_BACKGROUND\": USE_BACKGROUND,\n",
        "                    \"VIDEO_STEPS\": VIDEO_STEPS,\n",
        "                    \"TRACE_EVERY\": TRACE_EVERY}\n",
        "  metadata[\"prompt\"] = metadata_prompt\n",
        "\n",
        "  # Write metadata to a Python-interpretable text file.\n",
        "  with open(metadata_filename, \"w\") as f:\n",
        "    for config_key, config in metadata.items():\n",
        "      f.write(f\"# {config_key}\\n\")\n",
        "      for key, value in config.items():\n",
        "        if isinstance(value, str):\n",
        "          f.write(f\"{key} = \\\"{value}\\\"\\n\")\n",
        "        else:\n",
        "          f.write(f\"{key} = {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpPJx-r_QZq3"
      },
      "source": [
        "# Training and evolution function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApgzwCVJcU7v",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Image augmentation transformations for training\n",
        "\n",
        "def augmentation_transforms(canvas_width,\n",
        "                            use_normalized_clip=False,\n",
        "                            use_augmentation=False):\n",
        "  \"\"\"Image transforms to produce distorted crops to augment the evaluation.\n",
        "\n",
        "  Args:\n",
        "    canvas_width: width of the drawing canvas\n",
        "    use_normalized_clip: Normalisation to better suit CLIP's training data\n",
        "    use_augmentation: Image augmentation by affine transform\n",
        "\n",
        "  Returns:\n",
        "    transforms\n",
        "  \"\"\"\n",
        "  if use_normalized_clip and use_augmentation:\n",
        "    augment_trans = transforms.Compose(\n",
        "        [transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.6),\n",
        "         transforms.RandomResizedCrop(canvas_width, scale=(0.7, 0.9)),\n",
        "         transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                              (0.26862954, 0.26130258, 0.27577711))])\n",
        "  elif use_augmentation:\n",
        "    augment_trans = transforms.Compose([\n",
        "        transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.6),\n",
        "        transforms.RandomResizedCrop(canvas_width, scale=(0.7, 0.9)),\n",
        "    ])\n",
        "  elif use_normalized_clip:\n",
        "    augment_trans = transforms.Normalize(\n",
        "        (0.48145466, 0.4578275, 0.40821073),\n",
        "        (0.26862954, 0.26130258, 0.27577711))\n",
        "  else:\n",
        "    augment_trans = transforms.RandomPerspective(\n",
        "        fill=1, p=0, distortion_scale=0)\n",
        "\n",
        "  return augment_trans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4XIVMSJuWgxG"
      },
      "outputs": [],
      "source": [
        "#@title Training functions\n",
        "\n",
        "def moving_average(a, n=3) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "\n",
        "def plot_and_save_losses(loss_history, title=\"Losses\", filename=None):\n",
        "  losses = np.array(loss_history)\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.xlabel(\"Training steps\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(title)\n",
        "  plt.plot(moving_average(losses, n=3))\n",
        "  if filename:\n",
        "    np.save(filename + \".npy\", losses, allow_pickle=True)\n",
        "    plt.savefig(filename + \".png\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def make_optimizer(generator, learning_rate):\n",
        "  \"\"\"Make optimizer for generator's parameters.\n",
        "\n",
        "  Args:\n",
        "    generator: generator model\n",
        "    learning_rate: learning rate\n",
        "    input_learing_rate: learning rate for input\n",
        "\n",
        "  Returns:\n",
        "    optimizer\n",
        "  \"\"\"\n",
        "\n",
        "  my_list = ['positions_top']\n",
        "  params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in my_list, \n",
        "                                               generator.named_parameters()))))\n",
        "  base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in \n",
        "                                      my_list, generator.named_parameters()))))\n",
        "  lr_scheduler = torch.optim.SGD([{'params': base_params},\n",
        "                                  {'params': params, 'lr': learning_rate}],\n",
        "                                    lr=learning_rate)\n",
        "  return lr_scheduler\n",
        "\n",
        "\n",
        "def text_features(prompts):\n",
        "  # Compute CLIP features for all prompts.\n",
        "  text_inputs = []\n",
        "  for prompt in prompts:\n",
        "    text_inputs.append(clip.tokenize(prompt).to(device))\n",
        "\n",
        "  features = []\n",
        "  with torch.no_grad():\n",
        "    for text_input in text_inputs:\n",
        "      features.append(clip_model.encode_text(text_input))\n",
        "  return features\n",
        "\n",
        "\n",
        "def create_augmented_batch(images, augment_trans, text_features):\n",
        "  \"\"\"Create batch of images to be evaluated.\n",
        "  \n",
        "  Returns:\n",
        "    img_batch: For compositional images the batch contains all the regions.\n",
        "        Otherwise the batch contains augmented versions of the original images.\n",
        "    num_augs: number of images per original image\n",
        "    expanded_text_features: a text feature for each augmentation\n",
        "    loss_weights: weights for loss associated with each augmentation image\n",
        "  \"\"\"\n",
        "  images = images.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  expanded_text_features = []\n",
        "  if USE_IMAGE_AUGMENTATIONS:\n",
        "    num_augs = NUM_AUGS\n",
        "    img_augs = [] \n",
        "    for n in range(NUM_AUGS):\n",
        "      img_n = augment_trans(images)\n",
        "      img_augs.append(img_n)\n",
        "      expanded_text_features.append(text_features[0])\n",
        "    img_batch = torch.cat(img_augs)\n",
        "    # Given images [P0, P1] and augmentations [a0(), a1()], output format:\n",
        "    # [a0(P0), a0(P1), a1(P0), a1(P1)]\n",
        "  else:\n",
        "    num_augs = 1\n",
        "    if USE_NORMALIZED_CLIP:\n",
        "      img_batch = augment_trans(images)\n",
        "    else:\n",
        "      img_batch = images\n",
        "    expanded_text_features.append(text_features[0])\n",
        "  return img_batch, num_augs, expanded_text_features, [1] * NUM_AUGS\n",
        "\n",
        "\n",
        "def create_compositional_batch(images, augment_trans, text_features):\n",
        "  \"\"\"Create 10 sub-images per image by augmenting each with 3x3 crops.\n",
        "\n",
        "  Args:\n",
        "    images: population of N images, format [N, C, H, W]\n",
        "\n",
        "  Returns:\n",
        "    Tensor of all compositional sub-images + originals; [N*10, C, H, W] format:\n",
        "        [x0_y0(P0) ... x0_y0(PN), ..., x2_y2(P0) ... x2_y2(PN), P0, ..., PN]\n",
        "    10: Number of sub-images + whole, per original image.\n",
        "    expanded_text_features: list of text features, 1 for each composition image\n",
        "    loss_weights: weights for the losses corresponding to each composition image\n",
        "    \"\"\"\n",
        "  if len(text_features) != 10:\n",
        "    # text_features should already be 10 in size.\n",
        "    raise ValueError(\n",
        "        \"10 text prompts required for compositional image creation\")\n",
        "  resize_for_clip = transforms.Compose([transforms.Scale((224,224))])\n",
        "  img_swap = torch.swapaxes(images, 3, 1)\n",
        "  ims = []\n",
        "  i = 0\n",
        "  for x in range(3):\n",
        "    for y in range(3):\n",
        "      #  print(f\"Prompt for x={x}, y={y} is {PROMPTS[i]}\")\n",
        "      for k in range(images.shape[0]):\n",
        "        ims.append(resize_for_clip(\n",
        "            img_swap[k][:, y * 112 : y * 112 + 224, x * 112 : x * 112 + 224]))  \n",
        "      i += 1\n",
        "  \n",
        "  # Top-level (whole) images\n",
        "  for k in range(images.shape[0]):\n",
        "    ims.append(resize_for_clip(img_swap[k]))\n",
        "  all_img = torch.stack(ims)\n",
        "  all_img = torch.swapaxes(all_img, 1, 3)\n",
        "  all_img = all_img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  all_img = augment_trans(all_img)\n",
        "\n",
        "  # Last image gets 9 times as much weight\n",
        "  common_weight = 1 / 5\n",
        "  loss_weights = [common_weight] * 9\n",
        "  loss_weights.append(9 * common_weight)\n",
        "  return all_img, 10, text_features, loss_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LFko54cGSkai"
      },
      "outputs": [],
      "source": [
        "#@title Evaluation and step of optimization\n",
        "\n",
        "# Show each image being evaluated for debugging purposes.\n",
        "VISUALISE_BATCH_IMAGES = False\n",
        "\n",
        "def evaluation(t, clip_enc, generator, augment_trans, text_features):\n",
        "  \"\"\"Do a step of evaluation, returning images and losses.\n",
        "\n",
        "  Args:\n",
        "    t: step count\n",
        "    clip_enc: model for CLIP encoding\n",
        "    generator: drawing generator to optimise\n",
        "    augment_trans: transforms for image augmentation\n",
        "    text_features: tuple with the prompt two negative prompts\n",
        "  \n",
        "  Returns:\n",
        "    loss: torch.Tensor of single combines loss\n",
        "    losses_separate_np: numpy array of loss for each image\n",
        "    losses_individuals_np: numpy array with loss for each population individual\n",
        "    img_np: numpy array of images from the generator\n",
        "  \"\"\"\n",
        "\n",
        "  # Annealing parameters.\n",
        "  params = {'gamma': t / OPTIM_STEPS}\n",
        "\n",
        "  # Rebuild the generator.\n",
        "  img = generator(params)\n",
        "  img_np = img.detach().cpu().numpy()\n",
        "\n",
        "  # Create images for different regions\n",
        "  pop_size = img.shape[0]\n",
        "  if COMPOSITIONAL_IMAGE:\n",
        "    (img_batch, num_augs, text_features, loss_weights\n",
        "     ) = create_compositional_batch(img, augment_trans, text_features)\n",
        "  else:\n",
        "    (img_batch, num_augs, text_features, loss_weights\n",
        "     ) = create_augmented_batch(img, augment_trans, text_features)\n",
        "  losses = torch.zeros(pop_size, num_augs).to(device)\n",
        "\n",
        "  # Compute and add losses after augmenting the image with transforms.\n",
        "  img_batch = torch.clip(img_batch, 0, 1)  # clip the images.\n",
        "  image_features = clip_enc.encode_image(img_batch)\n",
        "  count = 0\n",
        "  for n in range(num_augs):  # number of augmentations or composition images\n",
        "    for p in range(pop_size):\n",
        "      loss = torch.cosine_similarity(\n",
        "          text_features[n], image_features[count:count+1], dim=1\n",
        "          )[0] * loss_weights[n]\n",
        "      losses[p, n] -= loss\n",
        "      if VISUALISE_BATCH_IMAGES and t % 500 == 0:\n",
        "        # Show all the images in the batch along with their losses.\n",
        "        if COMPOSITIONAL_IMAGE:\n",
        "          print(f\"Loss {loss} for image region with prompt {PROMPTS[n]}:\")\n",
        "        else:\n",
        "          print(f\"Loss {loss} for image augmentation with prompt {PROMPTS[0]}:\")\n",
        "        show_and_save(img_batch[count].unsqueeze(0), img_format=\"SCHW\")\n",
        "      count += 1\n",
        "  loss = torch.sum(losses) / pop_size\n",
        "  losses_separate_np = losses.detach().cpu().numpy()\n",
        "  # Sum losses for all each population individual.\n",
        "  losses_individuals_np = losses_separate_np.sum(axis=1)\n",
        "  return loss, losses_separate_np, losses_individuals_np, img_np\n",
        "\n",
        "\n",
        "def step_optimization(t, clip_enc, lr_scheduler, generator, augment_trans,\n",
        "                      text_features, final_step=False):\n",
        "  \"\"\"Do a step of optimization.\n",
        "\n",
        "  Args:\n",
        "    t: step count\n",
        "    clip_enc: model for CLIP encoding\n",
        "    lr_scheduler: optimizer\n",
        "    generator: drawing generator to optimise\n",
        "    augment_trans: transforms for image augmentation\n",
        "    text_features: list or 1 or 9 prompts for normal and compositional creation\n",
        "    final_step: if True does extras such as saving the model\n",
        "\n",
        "  Returns:\n",
        "    losses_np: numpy array with loss for each population individual\n",
        "    losses_separate_np: numpy array of loss for each image\n",
        "  \"\"\"\n",
        "\n",
        "  # Anneal learning rate and other parameters.\n",
        "  if t == int(OPTIM_STEPS / 3):\n",
        "    for g in lr_scheduler.param_groups:\n",
        "      g[\"lr\"] = g[\"lr\"] / 2.0\n",
        "  if t == int(OPTIM_STEPS * (2/3)):\n",
        "    for g in lr_scheduler.param_groups:\n",
        "      g[\"lr\"] = g[\"lr\"] / 2.0\n",
        "  params = {'gamma': t / OPTIM_STEPS}\n",
        "\n",
        "  # Forward pass. \n",
        "  lr_scheduler.zero_grad()\n",
        "  loss, losses_separate_np, losses_np, img_np = evaluation(\n",
        "      t, clip_enc, generator, augment_trans, text_features)\n",
        "\n",
        "  # Backpropagate the gradients.\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm(generator.parameters(), GRADIENT_CLIPPING)\n",
        "  \n",
        "  # Decay the learning rate.\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  # Render the big version.\n",
        "  if final_step:\n",
        "    show_and_save(img_np, t=t, img_format=\"SHWC\")\n",
        "    print(\"Saving model...\")\n",
        "    torch.save(generator.state_dict(), DIR_RESULTS + \"/generator.pt\")\n",
        "\n",
        "  if t % TRACE_EVERY == 0:\n",
        "    img_stitched = show_and_save(img_np, \n",
        "                                 max_display=MAX_MULTIPLE_VISUALISATIONS,\n",
        "                                 stitch=True, img_format=\"SHWC\")\n",
        "\n",
        "    print(\"Iteration {:3d}, rendering loss {:.6f}\".format(t, loss.item()))\n",
        "  return losses_np, losses_separate_np, img_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XlcTY0mjmQ5W"
      },
      "outputs": [],
      "source": [
        "#@title Evolution functions\n",
        "\n",
        "def population_evolution_step(generator, losses):\n",
        "  \"\"\"GA for the population.\"\"\"\n",
        "  if GA_METHOD == \"Microbial\":\n",
        "    # Competition between 2 random individuals; mutated winner replaces loser.\n",
        "    indices = list(range(len(losses)))\n",
        "    random.shuffle(indices)\n",
        "    select_1, select_2 = indices[0], indices[1]\n",
        "    if losses[select_1] < losses[select_2]:\n",
        "      # print(f\"Replacing {select_2} with {select_1}\")\n",
        "      generator.copy_and_mutate_s(select_1, select_2)\n",
        "    else:\n",
        "      # print(f\"Replacing {select_1} with {select_2}\")\n",
        "      generator.copy_and_mutate_s(select_2, select_1)\n",
        "  elif GA_METHOD == \"Evolutionary Strategies\":\n",
        "    # Replace rest of population with mutants of the best.\"\"\"\n",
        "    winner = np.argmin(losses)\n",
        "    for other in range(len(losses)):\n",
        "      if other == winner:\n",
        "        continue\n",
        "      generator.copy_and_mutate_s(winner, other)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyyw6Glris_e"
      },
      "source": [
        "# Make Collages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CollageMaker class definition.\n",
        "\n",
        "class CollageMaker():\n",
        "  def __init__(self, #prompts, \n",
        "               segmented_data, \n",
        "               background_image,\n",
        "               output_dir,\n",
        "               file_basename, \n",
        "               video_steps,\n",
        "               population_video):\n",
        "    #self._prompts = prompts\n",
        "    self._segmented_data = segmented_data\n",
        "    self._background_image = background_image\n",
        "    self._file_basename = file_basename\n",
        "    self._output_dir = output_dir\n",
        "    self._population_video = population_video\n",
        "\n",
        "    self._video_steps = video_steps\n",
        "    if self._video_steps:\n",
        "      self._video_writer = VideoWriter(\n",
        "          filename=f\"{self._output_dir}/{self._file_basename}.mp4\")\n",
        "      if self._population_video:\n",
        "        self._population_video_writer = VideoWriter(\n",
        "            filename=f\"{self._output_dir}/{self._file_basename}_pop_sample.mp4\")\n",
        "    \n",
        "    if COMPOSITIONAL_IMAGE:\n",
        "      print(\"Global prompt is\", PROMPTS[-1])\n",
        "      print(\"Composition prompts\", PROMPTS)\n",
        "    else:\n",
        "      print(\"CLIP prompt\", PROMPTS)\n",
        "    \n",
        "    # Prompt to CLIP features.\n",
        "    self._prompt_features = text_features(PROMPTS)\n",
        "    self._augmentations = augmentation_transforms(\n",
        "        224,\n",
        "        use_normalized_clip=USE_NORMALIZED_CLIP,\n",
        "        use_augmentation=USE_IMAGE_AUGMENTATIONS)\n",
        "    \n",
        "    # Create population of collage generators.\n",
        "    self._generator = PopulationCollage(\n",
        "        is_high_res=False,\n",
        "        pop_size=POP_SIZE,\n",
        "        segmented_data=segmented_data,\n",
        "        background_image=background_image)\n",
        "    \n",
        "    # Initial search over hyper-parameters.\n",
        "    if INITIAL_SEARCH_SIZE > 1:\n",
        "      print(f'\\nInitial random search over {INITIAL_SEARCH_SIZE} individuals')\n",
        "      for j in range(POP_SIZE):\n",
        "        generator_search = PopulationCollage(\n",
        "            is_high_res=False,\n",
        "            pop_size=INITIAL_SEARCH_SIZE,\n",
        "            segmented_data=segmented_data,\n",
        "            background_image=background_image)\n",
        "        _, _, losses, _ = evaluation(\n",
        "            0, clip_model, generator_search, augmentations, prompt_features)\n",
        "        print(f\"Search {losses}\")\n",
        "        idx_best = np.argmin(losses)\n",
        "        generator.copy_from(generator_search, j, idx_best)\n",
        "        del generator_search\n",
        "      print(f'Initial random search done\\n')\n",
        "    \n",
        "    self._optimizer = make_optimizer(self._generator, LEARNING_RATE)\n",
        "    self._step = 0\n",
        "    self._losses_history = []\n",
        "    self._losses_separated_history = []\n",
        "\n",
        "  @property\n",
        "  def generator(self):\n",
        "    return self._generator\n",
        "    \n",
        "  @property\n",
        "  def step(self):\n",
        "    return self._step\n",
        "\n",
        "  def loop(self):\n",
        "    \"\"\"Main optimisation/image generation loop. Can be interrupted.\"\"\"\n",
        "    if self._step == 0:\n",
        "      print('Starting optimization of collage.')\n",
        "    else:\n",
        "      print(f'Continuing optimization of collage at step {self._step}.')\n",
        "      if self._video_steps:\n",
        "        print(f\"Aborting video creation (does not work when interrupted).\")\n",
        "        self._video_steps = 0\n",
        "        self._video_writer = None\n",
        "        if self._population_video_writer:\n",
        "          self._population_video_writer = None\n",
        "    \n",
        "    while self._step < OPTIM_STEPS:\n",
        "      last_step = self._step == (OPTIM_STEPS - 1)\n",
        "      losses, losses_separated, img_batch = step_optimization(\n",
        "          self._step, clip_model, self._optimizer, self._generator,\n",
        "          self._augmentations, self._prompt_features, final_step=last_step)\n",
        "      self._add_video_frames(img_batch, losses)\n",
        "      self._losses_history.append(losses)\n",
        "      self._losses_separated_history.append(losses_separated)\n",
        "    \n",
        "      if USE_EVOLUTION and self._step and self._step % EVOLUTION_FREQUENCY == 0:\n",
        "        population_evolution_step(self._generator, losses) \n",
        "      self._step += 1\n",
        "\n",
        "  def high_res_render(self, \n",
        "                      segmented_data_high_res, \n",
        "                      background_image_high_res,\n",
        "                      gamma=1.0, \n",
        "                      show=True):\n",
        "    \"\"\"Save and/or show a high res render using high-res patches.\"\"\"\n",
        "    generator = PopulationCollage(\n",
        "        is_high_res=True,\n",
        "        pop_size=1,\n",
        "        segmented_data=segmented_data_high_res,\n",
        "        background_image=background_image_high_res)\n",
        "    idx_best = np.argmin(self._losses_history[-1])\n",
        "    print(f'Lowest loss for indices: {idx_best}')\n",
        "    generator.copy_from(self._generator, 0, idx_best)\n",
        "    # Show high res version given a generator\n",
        "    generator_cpu = copy.deepcopy(generator)\n",
        "    generator_cpu = generator_cpu.to('cpu')\n",
        "    generator_cpu.tensors_to('cpu')\n",
        "  \n",
        "    params = {'gamma': gamma}\n",
        "    with torch.no_grad():\n",
        "      img_high_res = generator_cpu.forward(params)\n",
        "    img = img_high_res.detach().cpu().numpy()[0]\n",
        "  \n",
        "    # Swap Red with Blue\n",
        "    img = img[...,[2, 1, 0]]  \n",
        "    img = np.clip(img, 0.0, 1.0) * 255\n",
        "    image_filename = f\"{self._output_dir}/{self._file_basename}.png\"\n",
        "    cv2.imwrite(image_filename, img)\n",
        "    \n",
        "    if show:\n",
        "      cv2_imshow(img)\n",
        "      cv2.waitKey()\n",
        "\n",
        "  def finish(self):\n",
        "    \"\"\"Finish video writing and save all other data.\"\"\"\n",
        "    if self._losses_history:\n",
        "      losses_filename = f\"{self._output_dir}/{self._file_basename}_losses\"\n",
        "      plot_and_save_losses(self._losses_history, \n",
        "                           title=f\"{PROMPT} ({RENDER_METHOD}) Losses\",\n",
        "                           filename=losses_filename)\n",
        "    if self._video_steps:\n",
        "      self._video_writer.close()\n",
        "    if self._population_video:\n",
        "      self._population_video_writer.close()\n",
        "    metadata_filename = f\"{self._output_dir}/{self._file_basename}_metadata.py\"\n",
        "    export_metadata(metadata_filename)\n",
        "\n",
        "  def _add_video_frames(self, img_batch, losses):\n",
        "    \"\"\"Add images from numpy image batch to video writers.\n",
        "    \n",
        "    Args:\n",
        "      img_batch: numpy array, batch of images (S,H,W,C)\n",
        "      losses: numpy array, losses for each generator (S,N)\n",
        "    \"\"\"\n",
        "    if self._video_steps and self._step % self._video_steps == 0:\n",
        "      # Write image to video.\n",
        "      best_img = img_batch[np.argmin(losses)]\n",
        "      self._video_writer.add(cv2.resize(\n",
        "          best_img, (best_img.shape[1] * 3, best_img.shape[0] * 3)))\n",
        "      if self._population_video:\n",
        "        laid_out = layout_img_batch(img_batch)\n",
        "        self._population_video_writer.add(cv2.resize(\n",
        "            laid_out, (laid_out.shape[1] * 2, laid_out.shape[0] * 2)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oHGr2ogDRBMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6drbSbshNN0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Configure image prompt and content\n",
        "\n",
        "#@markdown Enter a description of the image, e.g. 'a photorealistic chicken'\n",
        "# PROMPT = \"A photorealistic chicken\"  #@param {type:\"string\"}\n",
        "\n",
        "PROMPT = \"A photorealistic landscape\"   #@param {type:\"string\"}\n",
        "\n",
        "FILE_BASENAME = \"landscape_composition\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Use an uploaded image or solid colour as background instead of black\n",
        "USE_BACKGROUND = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Use solid colour instead of an image?\n",
        "USE_SOLID_COLOUR_BACKGROUND = True #@param {type:\"boolean\"}\n",
        "BACKGROUND_RED = 195 #@param {type:\"slider\", min:0, max:255, step:1}\n",
        "BACKGROUND_GREEN = 181 #@param {type:\"slider\", min:0, max:255, step:1}\n",
        "BACKGROUND_BLUE = 172 #@param {type:\"slider\", min:0, max:255, step:1}\n",
        "\n",
        "#@markdown Debugging and monitoring\n",
        "VIDEO_STEPS = 10  #@param {type:\"integer\"}\n",
        "TRACE_EVERY = 100  #@param {type:\"integer\"}\n",
        "\n",
        "PROMPTS = [PROMPT]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure region prompts\n",
        "\n",
        "#@markdown If using 3x3 compositional image specify prompt for each region (left to right, starting at the top)\n",
        "PROMPT_x0_y0 = \"a photorealistic sky with sun\"   #@param {type:\"string\"}\n",
        "PROMPT_x1_y0 = \"a photorealistic sky\"   #@param {type:\"string\"}\n",
        "PROMPT_x2_y0 = \"a photorealistic sky with moon\"   #@param {type:\"string\"}\n",
        "PROMPT_x0_y1 = \"a photorealistic tree\"   #@param {type:\"string\"}\n",
        "PROMPT_x1_y1 = \"a photorealistic tree\"   #@param {type:\"string\"}\n",
        "PROMPT_x2_y1 = \"a photorealistic tree\"   #@param {type:\"string\"}\n",
        "PROMPT_x0_y2 = \"a photorealistic field\"   #@param {type:\"string\"}\n",
        "PROMPT_x1_y2 = \"a photorealistic field\"   #@param {type:\"string\"}\n",
        "PROMPT_x2_y2 = \"a photorealistic chicken\"   #@param {type:\"string\"}\n",
        "\n",
        "if COMPOSITIONAL_IMAGE:\n",
        "  PROMPTS = [PROMPT_x0_y0, PROMPT_x1_y0, PROMPT_x2_y0, PROMPT_x0_y1, PROMPT_x1_y1, PROMPT_x2_y1, PROMPT_x0_y2, PROMPT_x1_y2, PROMPT_x2_y2, PROMPT]"
      ],
      "metadata": {
        "id": "Iz10BbmO4w5n",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYQm8iZv12N-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Get background image (if using one)\n",
        "\n",
        "def upload_files():\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    open(k, 'wb').write(v)\n",
        "  return list(uploaded.keys())\n",
        "\n",
        "def _resize_background(img, canvas_height, canvas_width):\n",
        "  img = torch.tensor(resize(img.astype(float),\n",
        "                            (canvas_height, canvas_width))).cuda()\n",
        "  return img.permute(2, 0, 1).to(torch.float32) / 255.0\n",
        "\n",
        "background_image = None\n",
        "background_image_high_res = None\n",
        "\n",
        "if USE_BACKGROUND:\n",
        "  if not USE_SOLID_COLOUR_BACKGROUND:\n",
        "    backgrounds = upload_files()\n",
        "    img = cv2.imread(backgrounds[0])\n",
        "    cv2_imshow(img)\n",
        "    cv2.waitKey()\n",
        "    \n",
        "    # Convert to numpy array for use. \n",
        "    background_image_np = np.asarray(img)\n",
        "    background_image_np = background_image_np[..., ::-1].copy()\n",
        "    print('Loaded background image')\n",
        "  else:\n",
        "    background_image_np = np.ones((10, 10, 4), dtype=np.float32)\n",
        "    background_image_np[:, :, 0] = BACKGROUND_RED\n",
        "    background_image_np[:, :, 1] = BACKGROUND_GREEN\n",
        "    background_image_np[:, :, 2] = BACKGROUND_BLUE\n",
        "    background_image_np[:, :, 3] = 255.\n",
        "    print('Defined background colour ({}, {}, {})'.format(\n",
        "        BACKGROUND_RED, BACKGROUND_GREEN, BACKGROUND_BLUE))\n",
        "\n",
        "  background_image = _resize_background(\n",
        "      background_image_np, CANVAS_HEIGHT, CANVAS_WIDTH)\n",
        "  background_image_high_res = _resize_background(\n",
        "      background_image_np,\n",
        "      CANVAS_HEIGHT * MULTIPLIER_BIG_IMAGE,\n",
        "      CANVAS_WIDTH * MULTIPLIER_BIG_IMAGE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create collage! (Initialisation)\n",
        "\n",
        "OUTPUT_DIR = f\"{FILE_BASENAME}_results\"\n",
        "!mkdir {OUTPUT_DIR}\n",
        "!rm {OUTPUT_DIR}/*\n",
        "\n",
        "# Prepare the outputs.\n",
        "zipname = f\"{FILE_BASENAME}_rendering.zip\"\n",
        "print(f\"Output to {OUTPUT_DIR}, zipfile will be saved to {zipname}\")\n",
        "file_basename = f\"{FILE_BASENAME}_{RENDER_METHOD}\"\n",
        "\n",
        "collage_maker = CollageMaker(#prompts, \n",
        "               segmented_data=segmented_data, background_image=background_image,\n",
        "               output_dir=OUTPUT_DIR, file_basename=file_basename, \n",
        "               video_steps=VIDEO_STEPS, population_video=POPULATION_VIDEO)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YVst6OhPVx_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create collage! (Main loop)\n",
        "\n",
        "#@markdown To edit patches interrupt this cell and run the one below this. Re-run this cell afterwards to continue generating the image.\n",
        "collage_maker.loop()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rJ-ESoz8eekT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tinker with patches\n",
        "#@markdown Enable this cell to allow patch editing:\n",
        "PATCH_TINKERING = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Interupt the cell above mid-optimisation and run this cell to manually adjust the patches. Run it several times to adjust different patches. Then re-run the cell above to continue optimising.\n",
        "\n",
        "if PATCH_TINKERING:\n",
        "  from ipywidgets import interactive\n",
        "  import IPython.display\n",
        "  from google.colab.output import eval_js\n",
        "  import base64\n",
        "  \n",
        "  # Render the current collage(s).\n",
        "  generator = collage_maker.generator\n",
        "  step = collage_maker.step\n",
        "  params = {'gamma': step / OPTIM_STEPS}\n",
        "  img = generator(params)\n",
        "  img = img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  print('Current collage(s)')\n",
        "  res_img = show_and_save(img, t=step,\n",
        "                          max_display=MAX_MULTIPLE_VISUALISATIONS,\n",
        "                          stitch=True, show=False)\n",
        "  filename_temp = f\"./temp.png\"\n",
        "  res_img = cv2.cvtColor(res_img, cv2.COLOR_BGR2RGB) * 255\n",
        "  cv2.imwrite(filename_temp, res_img)\n",
        "  \n",
        "  # HTML code to plot the image and detect the mouse cursor.\n",
        "  canvas_html = \"\"\"\n",
        "  <canvas width=%d height=%d></canvas>\n",
        "  <script>\n",
        "    var filename_image = \"%s\"\n",
        "    var canvas = document.querySelector('canvas')\n",
        "    var ctx = canvas.getContext('2d')\n",
        "    ctx.lineWidth = 1\n",
        "    var mouse = {x: 0, y: 0}\n",
        "    canvas.addEventListener('mousemove', function(e) {\n",
        "      mouse.x = e.pageX - this.offsetLeft\n",
        "      mouse.y = e.pageY - this.offsetTop\n",
        "    })\n",
        "    canvas.onmousedown = ()=>{\n",
        "      ctx.beginPath()\n",
        "      ctx.moveTo(mouse.x, mouse.y)\n",
        "      canvas.addEventListener('mousemove', onPaint)\n",
        "    }\n",
        "    var onPaint = ()=>{\n",
        "      ctx.lineTo(mouse.x, mouse.y)\n",
        "      ctx.stroke()\n",
        "    }\n",
        "    var data = new Promise(resolve=>{\n",
        "      canvas.onmouseup = ()=>{\n",
        "        canvas.removeEventListener('mousemove', onPaint)\n",
        "        resolve(mouse)\n",
        "      }\n",
        "    })\n",
        "    function draw_collage_image() {\n",
        "      collage_image = new Image();\n",
        "      collage_image.src = filename_image;\n",
        "      collage_image.onload = function(){\n",
        "        ctx.drawImage(collage_image, 0, 0);\n",
        "      }\n",
        "    }\n",
        "    draw_collage_image();\n",
        "  </script>\n",
        "  \"\"\"\n",
        "  \n",
        "  im = IPython.display.Image(filename_temp, embed=True)\n",
        "  # IPython.display.display(im)\n",
        "  filename_embed = 'data:image/png;base64,'\n",
        "  filename_embed += base64.b64encode(im.data).decode('ascii')\n",
        "  \n",
        "  # Display an HTML canvas with the image.\n",
        "  canvas = IPython.display.HTML(\n",
        "      canvas_html % (CANVAS_WIDTH * POP_SIZE, CANVAS_HEIGHT, filename_embed))\n",
        "  print('Click with the mouse on the desired image and patch:')\n",
        "  IPython.display.display(canvas)\n",
        "  \n",
        "  # Select the image and pixel coordinates.\n",
        "  def draw():\n",
        "    print('draw()')\n",
        "    mouse = eval_js('data')\n",
        "    return mouse\n",
        "  mouse = draw()\n",
        "  pop_id_mouse = int(np.floor(mouse['x'] / CANVAS_WIDTH))\n",
        "  x_mouse = int(mouse['x'] % CANVAS_WIDTH)\n",
        "  y_mouse = int(mouse['y'])\n",
        "  print(f'Selected image {pop_id_mouse} at ({x_mouse}, {y_mouse})')\n",
        "  \n",
        "  def find_patch(generator, id, u, v):\n",
        "    # Render only the spatial transforms of the patches.\n",
        "    rendered_patches = generator.spatial_transformer(generator.patches)\n",
        "    rendered_patches = rendered_patches.detach().cpu().numpy()\n",
        "    patch_id = np.argmax(rendered_patches[id, :, 3, u, v] * rendered_patches[id, :, 4, u, v])\n",
        "    return patch_id\n",
        "  \n",
        "  # Select the patch.\n",
        "  patch_id = find_patch(generator, pop_id_mouse, y_mouse, x_mouse)\n",
        "  print(f'Found matching patch {patch_id}')\n",
        "  \n",
        "  # Extract the patch's current affine transform paramaters.\n",
        "  with torch.no_grad():\n",
        "    x0 = generator.spatial_transformer.translation[pop_id_mouse, patch_id, 0, 0]\n",
        "    x0 = float(x0.detach().cpu().numpy())\n",
        "    y0 = generator.spatial_transformer.translation[pop_id_mouse, patch_id, 1, 0]\n",
        "    y0 = float(y0.detach().cpu().numpy())\n",
        "    rot0 = generator.spatial_transformer.rotation[pop_id_mouse, patch_id, 0, 0]\n",
        "    rot0 = float(rot0.detach().cpu().numpy())\n",
        "    scale0 = generator.spatial_transformer.scale[pop_id_mouse, patch_id, 0, 0]\n",
        "    scale0 = float(scale0.detach().cpu().numpy())\n",
        "    squeeze0 = generator.spatial_transformer.squeeze[pop_id_mouse, patch_id, 0, 0]\n",
        "    squeeze0 = float(squeeze0.detach().cpu().numpy())\n",
        "    shear0 = generator.spatial_transformer.shear[pop_id_mouse, patch_id, 0, 0]\n",
        "    shear0 = float(shear0.detach().cpu().numpy())\n",
        "    patch_info = {'pop_id': pop_id_mouse, 'patch_id': patch_id,\n",
        "                  'x0': x0, 'y0': y0, 'rot0': rot0,\n",
        "                  'scale0': scale0, 'squeeze0': squeeze0, 'shear0': shear0,\n",
        "                  'x': x0, 'y': y0, 'rot': rot0,\n",
        "                  'scale': scale0, 'squeeze': squeeze0, 'shear': shear0}\n",
        "  \n",
        "  def show_modified(dx, dy, drot, dscale, dsqueeze, dshear):\n",
        "    \"\"\"Visualization callback function with affine transform deltas.\"\"\"\n",
        "    with torch.no_grad():\n",
        "      x = patch_info['x0'] - dx\n",
        "      y = patch_info['y0'] + dy\n",
        "      rot = patch_info['rot0'] - drot\n",
        "      scale = patch_info['scale0'] - dscale\n",
        "      squeeze = patch_info['squeeze0'] + dsqueeze\n",
        "      shear = patch_info['shear0'] + dshear\n",
        "      generator.spatial_transformer.translation[pop_id_mouse, patch_id, 0, 0] = x\n",
        "      generator.spatial_transformer.translation[pop_id_mouse, patch_id, 1, 0] = y\n",
        "      generator.spatial_transformer.rotation[pop_id_mouse, patch_id, 0, 0] = rot\n",
        "      generator.spatial_transformer.scale[pop_id_mouse, patch_id, 0, 0] = scale\n",
        "      generator.spatial_transformer.squeeze[pop_id_mouse, patch_id, 0, 0] = squeeze\n",
        "      generator.spatial_transformer.shear[pop_id_mouse, patch_id, 0, 0] = shear\n",
        "    patch_info['x'] = x\n",
        "    patch_info['y'] = y\n",
        "    patch_info['rot'] = rot\n",
        "    patch_info['shear'] = shear\n",
        "    patch_info['squeeze'] = squeeze\n",
        "    patch_info['shear'] = shear\n",
        "    params = {'gamma': step / OPTIM_STEPS}\n",
        "    img = generator(params)\n",
        "    img = img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "    _ = show_and_save(img, t=step,\n",
        "                      max_display=MAX_MULTIPLE_VISUALISATIONS,\n",
        "                      stitch=True)\n",
        "  \n",
        "  # Interactive editing of the patch's affine transform parameters.\n",
        "  interactive_plot = interactive(show_modified,\n",
        "                                dx=(-MAX_TRANS * 2, MAX_TRANS * 2, 0.01),\n",
        "                                dy=(-MAX_TRANS * 2, MAX_TRANS * 2, 0.01),\n",
        "                                drot=(-MAX_ROT * 2, MAX_ROT * 2, 0.01),\n",
        "                                dscale=(-MAX_SCALE * 2, MAX_SCALE * 2, 0.01),\n",
        "                                dsqueeze=(-MAX_SQUEEZE * 2, MAX_SQUEEZE * 2, 0.01),\n",
        "                                dshear=(-MAX_SHEAR * 2, MAX_SHEAR * 2, 0.01))\n",
        "  output = interactive_plot.children[-1]\n",
        "  output.layout.height = '350px'\n",
        "else:\n",
        "  interactive_plot = \"Patch tinkering not enabled.\"\n",
        "interactive_plot\n",
        "  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "OZpKu3XcdogG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Render high res image and finish up.\n",
        "\n",
        "collage_maker.high_res_render(segmented_data_high_res,\n",
        "                              background_image_high_res)\n",
        "collage_maker.finish()\n",
        "\n",
        "# Download all the generated files.\n",
        "!zip -r {zipname} {OUTPUT_DIR}\n",
        "from google.colab import files\n",
        "files.download(zipname)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wlvI7Gm14HIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9biyGTC-6DOy"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G264zsJ0XV-3"
      },
      "outputs": [],
      "source": [
        "raise ValueError(\"Stop here.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1lQ8EKwObOD5"
      },
      "outputs": [],
      "source": [
        "#@title zip up outputs and download\n",
        "!zip -r {zipname} {OUTPUT_DIR}\n",
        "from google.colab import files\n",
        "files.download(zipname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c144I7cg6Hrh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Upload the image patches\n",
        "#@markdown Run this cell to upload a npy file containing segmented patches.\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  with open(fn, 'rb') as f:\n",
        "    segmented_data_initial = np.load(f, allow_pickle = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjYI9mBDHSxm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Creates patch file from PNGs\n",
        "#@markdown Colab needs to be changed to support local npy files.\n",
        "\n",
        "import imageio\n",
        "import glob\n",
        "\n",
        "TARGET_FILE = \"/content/patches.npy\"\n",
        "PNG_DIR = \"/content/pngs\"\n",
        "mkdir(PNG_DIR)\n",
        "\n",
        "def upload_files(target_path):\n",
        "  \"\"\"Upload files to target directory.\"\"\"\n",
        "  mkdir(target_path)\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    open(target_path + \"/\" + k, 'wb').write(v)\n",
        "  return list(uploaded.keys())\n",
        "\n",
        "png_imgs = []\n",
        "for png_im_path in glob.glob(PNG_DIR + \"/*.png\"):\n",
        "     png_im = imageio.imread(png_im_path)\n",
        "     print(png_im.shape)\n",
        "     png_imgs.append(png_im)\n",
        "\n",
        "png_imgs_np = np.array(png_imgs)\n",
        "np.save(TARGET_FILE, png_imgs_np, allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0jhLcGHz_6"
      },
      "source": [
        "#Scratch Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XleAKCQ850e_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Render image 0 and 1 with each rendering method\n",
        "\n",
        "rms = [\"opacity\", \"transparency\", \"masked_transparency\"]\n",
        "for rm in rms:\n",
        "  RENDER_METHOD = rm\n",
        "  print(rm)\n",
        "  t = 1\n",
        "  params = {'gamma': t}\n",
        "  img = generator(params)\n",
        "  img = img.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
        "  show_and_save(img, t=t, max_display=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure tiling\n",
        "# TILES_WIDTH = 1  #@param {type:\"slider\", min:1, max:8}\n",
        "# TILES_HEIGHT = 1  #@param {type:\"slider\", min:1, max:8}\n",
        "# TILES_PROMPT = \"\"  #@param {type:\"string\"}\n",
        "# \n",
        "# if not TILES_PROMPT\n",
        "#   TILES_PROMPT = PROMPT\n",
        "# \n",
        "# tiled_prompts = []\n",
        "# for x in range(TILES_WIDTH):\n",
        "#   for y in range(TILES_HEIGHT):\n",
        "#     tiled_prompts.append(PROMPTS)\n",
        "# PROMPTS = tiled_prompts"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Aa96frytD3t8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BfEKdIbK4VZa",
        "c8bdUyJs4hq3",
        "bTWV2a7ZekET",
        "HxR0ZFpAebsH",
        "r9GsjxMcgAP7",
        "ubwSr59fgk0C",
        "4ckYmVuAAO7x",
        "GIXzueO3PB-4",
        "IpPJx-r_QZq3",
        "Mu0jhLcGHz_6"
      ],
      "machine_shape": "hm",
      "name": "Arnheim_3_Collage_1_23.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}